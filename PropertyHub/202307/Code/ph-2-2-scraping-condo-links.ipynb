{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d55be6b",
   "metadata": {
    "papermill": {
     "duration": 0.00275,
     "end_time": "2023-07-27T11:00:05.679469",
     "exception": false,
     "start_time": "2023-07-27T11:00:05.676719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PropertyHub: 2.2 Scraping Condo Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884b10fd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-27T11:00:05.686126Z",
     "iopub.status.busy": "2023-07-27T11:00:05.685626Z",
     "iopub.status.idle": "2023-07-27T11:00:05.879089Z",
     "shell.execute_reply": "2023-07-27T11:00:05.878064Z"
    },
    "papermill": {
     "duration": 0.199996,
     "end_time": "2023-07-27T11:00:05.881834",
     "exception": false,
     "start_time": "2023-07-27T11:00:05.681838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83d5b1e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:00:05.889750Z",
     "iopub.status.busy": "2023-07-27T11:00:05.889039Z",
     "iopub.status.idle": "2023-07-27T11:00:06.034717Z",
     "shell.execute_reply": "2023-07-27T11:00:06.033616Z"
    },
    "papermill": {
     "duration": 0.152128,
     "end_time": "2023-07-27T11:00:06.036917",
     "exception": false,
     "start_time": "2023-07-27T11:00:05.884789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GetLinks():\n",
    "    def __init__(self):\n",
    "        self.delay_time = 0.1\n",
    "        \n",
    "        self.rent_place_type = []\n",
    "        self.rent_place_name = []\n",
    "        self.rent_link_name = []\n",
    "        self.rent_condo_links = []\n",
    "        self.sale_place_type = []\n",
    "        self.sale_place_name = []\n",
    "        self.sale_link_name = []\n",
    "        self.sale_condo_links = []\n",
    "        \n",
    "        self.retries = 5\n",
    "        self.backoff = 1     # time-out = [0.5, 1, 2, 4, 8]\n",
    "        self.status_forcelist = [403, 500, 502, 503, 504]\n",
    "        self.timeout = (10, 10)\n",
    "        \n",
    "        # Initialize request session for retries and timeout\n",
    "        self.s = requests.Session()\n",
    "        retries = Retry(total=self.retries,\n",
    "                        backoff_factor=self.backoff,\n",
    "                        status_forcelist=self.status_forcelist)\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    def import_shuffle_df(self, path, num):\n",
    "        self.location = pd.read_csv(path)\n",
    "        self.location = self.location[self.location['flag']==num]\n",
    "        \n",
    "    def get_condo_links(self, order):\n",
    "        df_place_link_name = self.location.iloc[order-1,:]\n",
    "        place_link_name = df_place_link_name['link_name']\n",
    "        \n",
    "        print(f\"Thread {order}: getting {df_place_link_name['num_rent']+df_place_link_name['num_sale']} links ...\")\n",
    "        \n",
    "        # Get for-rent links\n",
    "        if df_place_link_name['num_rent'] != 0:\n",
    "            try:\n",
    "                place_link = f'https://propertyhub.in.th/en/condo-for-rent/{place_link_name}'\n",
    "                # find max page number\n",
    "                soup = BeautifulSoup(self.s.get(place_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                find_max_page = soup.find_all(\"ul\", {\"class\": \"sc-1p20b44-0 IoRRS\"})\n",
    "                try:\n",
    "                    max_page = int(find_max_page[0].find_all('li')[-2].a['aria-label'].split()[-1])\n",
    "                except:\n",
    "                    max_page = 1\n",
    "                page_links = [place_link + f'/{i+1}' if i!=0 else place_link for i in range(max_page)]\n",
    "                for page_link in page_links:\n",
    "                    soup = BeautifulSoup(self.s.get(page_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                    find_condo_links = soup.select(\"a[href*='en/listings/']\")\n",
    "                    for link in find_condo_links:\n",
    "                        condo_link = 'https://propertyhub.in.th' + link['href']\n",
    "                        self.rent_place_type.append(df_place_link_name['place_type'])\n",
    "                        self.rent_place_name.append(df_place_link_name['place_name'])\n",
    "                        self.rent_link_name.append(df_place_link_name['link_name'])\n",
    "                        self.rent_condo_links.append(condo_link)\n",
    "                time.sleep(self.delay_time)\n",
    "            except:\n",
    "                print(f\"There is an invalid link: {place_link}\")\n",
    "\n",
    "        # Get for-sale links\n",
    "        if df_place_link_name['num_sale'] != 0:\n",
    "            try:\n",
    "                place_link = f'https://propertyhub.in.th/en/condo-for-sale/{place_link_name}'\n",
    "                # find max page number\n",
    "                soup = BeautifulSoup(self.s.get(place_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                find_max_page = soup.find_all(\"ul\", {\"class\": \"sc-1p20b44-0 IoRRS\"})\n",
    "                try:\n",
    "                    max_page = int(find_max_page[0].find_all('li')[-2].a['aria-label'].split()[-1])\n",
    "                except:\n",
    "                    max_page = 1\n",
    "                page_links = [place_link + f'/{i+1}' if i!=0 else place_link for i in range(max_page)]\n",
    "                for page_link in page_links:\n",
    "                    soup = BeautifulSoup(self.s.get(page_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                    find_condo_links = soup.select(\"a[href*='en/listings/']\")\n",
    "                    for link in find_condo_links:\n",
    "                        condo_link = 'https://propertyhub.in.th' + link['href']\n",
    "                        self.sale_place_type.append(df_place_link_name['place_type'])\n",
    "                        self.sale_place_name.append(df_place_link_name['place_name'])\n",
    "                        self.sale_link_name.append(df_place_link_name['link_name'])\n",
    "                        self.sale_condo_links.append(condo_link)\n",
    "                time.sleep(self.delay_time)\n",
    "            except:\n",
    "                print(f\"There is an invalid link: {place_link}\")\n",
    "\n",
    "        # Convert to df and remove duplicates\n",
    "        self.df_rent_condo_links = pd.DataFrame(self.rent_place_type,columns=['place_type'])\n",
    "        self.df_rent_condo_links['place_name'] = self.rent_place_name\n",
    "        self.df_rent_condo_links['link_name'] = self.rent_link_name\n",
    "        self.df_rent_condo_links['condo_link'] = self.rent_condo_links\n",
    "        self.df_rent_condo_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "        self.df_sale_condo_links = pd.DataFrame(self.sale_place_type,columns=['place_type'])\n",
    "        self.df_sale_condo_links['place_name'] = self.sale_place_name\n",
    "        self.df_sale_condo_links['link_name'] = self.sale_link_name\n",
    "        self.df_sale_condo_links['condo_link'] = self.sale_condo_links\n",
    "        self.df_sale_condo_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "        # Export\n",
    "        self.df_rent_condo_links.to_csv(f\"df_rent_condo_links_{order}.csv\",index=False)\n",
    "        self.df_sale_condo_links.to_csv(f\"df_sale_condo_links_{order}.csv\",index=False)\n",
    "        \n",
    "        print(f\"Thread {order}: finished !!!\")\n",
    "            \n",
    "    def join_df(self):\n",
    "        output_rent_files = glob.glob(os.getcwd()+'/df_rent*.csv')\n",
    "        output_sale_files = glob.glob(os.getcwd()+'/df_sale*.csv')\n",
    "        outputs_rent = [pd.read_csv(output_rent_file) for output_rent_file in output_rent_files]\n",
    "        outputs_sale = [pd.read_csv(output_sale_file) for output_sale_file in output_sale_files]\n",
    "\n",
    "        # Combine outputs\n",
    "        output_rent_all = outputs_rent[0]\n",
    "        for i in range(len(outputs_rent)-1):\n",
    "            output_rent_all = pd.concat([output_rent_all,outputs_rent[i+1]], axis=0, ignore_index=True)\n",
    "        output_sale_all = outputs_sale[0]\n",
    "        for i in range(len(outputs_sale)-1):\n",
    "            output_sale_all = pd.concat([output_sale_all,outputs_sale[i+1]], axis=0, ignore_index=True)\n",
    "            \n",
    "        # Drop duplicates\n",
    "        output_rent_all.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "        output_sale_all.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'!!!!!! Finished Scraping Links !!!!!!')\n",
    "        print(f'for-rent data len: {len(output_rent_all)}')\n",
    "        print(f'for-sale data len: {len(output_sale_all)}')\n",
    "\n",
    "        # Export the combine result\n",
    "        output_rent_all.to_csv(f\"{datetime.now().strftime('%Y%m')}_rent_condo_links_2.csv\",index=False)\n",
    "        output_sale_all.to_csv(f\"{datetime.now().strftime('%Y%m')}_sale_condo_links_2.csv\",index=False)\n",
    "\n",
    "        # Delete all the unused files\n",
    "        [os.remove(output_rent_file) for output_rent_file in output_rent_files];\n",
    "        [os.remove(output_sale_file) for output_sale_file in output_sale_files];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d84e8cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:00:06.043080Z",
     "iopub.status.busy": "2023-07-27T11:00:06.042715Z",
     "iopub.status.idle": "2023-07-27T13:21:11.409095Z",
     "shell.execute_reply": "2023-07-27T13:21:11.407727Z"
    },
    "papermill": {
     "duration": 8465.373063,
     "end_time": "2023-07-27T13:21:11.412372",
     "exception": false,
     "start_time": "2023-07-27T11:00:06.039309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of threads: 35\n",
      "Thread 1: getting 15915 links ...\n",
      "Thread 2: getting 15881 links ...\n",
      "Thread 3: getting 15842 links ...\n",
      "Thread 4: getting 15803 links ...\n",
      "Thread 5: getting 15764 links ...\n",
      "Thread 6: getting 15714 links ...\n",
      "Thread 7: getting 15595 links ...\n",
      "Thread 8: getting 15235 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/assumption-convent-silom-school\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/satri-woranat-bang-khen-school\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/santirat-institute-of-bussiness-adminstration\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/bangkok-christian-college\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/huai-khwang\n",
      "Thread 5: finished !!!\n",
      "Thread 9: getting 15018 links ...\n",
      "Thread 1: finished !!!\n",
      "Thread 10: getting 14892 links ...\n",
      "Thread 7: finished !!!\n",
      "Thread 11: getting 14800 links ...\n",
      "Thread 4: finished !!!\n",
      "Thread 12: getting 14401 links ...\n",
      "Thread 3: finished !!!\n",
      "Thread 13: getting 13924 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/thonburi-vocational-college\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/st-theresa-school\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/mrt-phetchaburi\n",
      "Thread 13: finished !!!\n",
      "Thread 14: getting 13525 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/samsen-wittayalai-school\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/assumption-commercial-college-a-c-c\n",
      "Thread 12: finished !!!\n",
      "Thread 15: getting 12863 links ...\n",
      "Thread 6: finished !!!\n",
      "Thread 16: getting 12801 links ...\n",
      "Thread 8: finished !!!\n",
      "Thread 17: getting 12322 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-sale/assumption-commercial-college-a-c-c\n",
      "Thread 11: finished !!!\n",
      "Thread 18: getting 12317 links ...\n",
      "Thread 2: finished !!!\n",
      "Thread 19: getting 12283 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-sale/siam-university\n",
      "Thread 10: finished !!!\n",
      "Thread 20: getting 12212 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/borom-ratchachonnani-bangkok-nursing-college\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-sale/wattana-wittaya-academy\n",
      "Thread 9: finished !!!\n",
      "Thread 21: getting 11963 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/suan-dusit-rajabhat-university\n",
      "Thread 15: finished !!!\n",
      "Thread 22: getting 11868 links ...\n",
      "Thread 18: finished !!!\n",
      "Thread 23: getting 11763 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/mrt-sukhumvit\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/bts-on-nut\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-sale/bts-phrom-phong\n",
      "Thread 14: finished !!!\n",
      "Thread 24: getting 11752 links ...\n",
      "Thread 21: finished !!!\n",
      "Thread 25: getting 11677 links ...\n",
      "Thread 23: finished !!!\n",
      "Thread 26: getting 11605 links ...\n",
      "Thread 20: finished !!!\n",
      "Thread 27: getting 11531 links ...\n",
      "Thread 19: finished !!!\n",
      "Thread 28: getting 11450 links ...\n",
      "Thread 17: finished !!!\n",
      "Thread 29: getting 11344 links ...\n",
      "Thread 16: finished !!!\n",
      "Thread 30: getting 11225 links ...\n",
      "Thread 22: finished !!!\n",
      "Thread 31: getting 11109 links ...\n",
      "Thread 26: finished !!!\n",
      "Thread 32: getting 10911 links ...\n",
      "Thread 30: finished !!!\n",
      "Thread 33: getting 10769 links ...\n",
      "Thread 24: finished !!!\n",
      "Thread 34: getting 10251 links ...\n",
      "Thread 29: finished !!!\n",
      "Thread 35: getting 10148 links ...\n",
      "Thread 27: finished !!!\n",
      "Thread 25: finished !!!\n",
      "Thread 28: finished !!!\n",
      "Thread 31: finished !!!\n",
      "Thread 34: finished !!!\n",
      "Thread 32: finished !!!\n",
      "Thread 33: finished !!!\n",
      "Thread 35: finished !!!\n",
      "!!!!!! Finished Scraping Links !!!!!!\n",
      "for-rent data len: 81639\n",
      "for-sale data len: 27036\n"
     ]
    }
   ],
   "source": [
    "num = 2\n",
    "input_path = '/kaggle/input/ph-1-getting-locations'\n",
    "file_name = 'locations.csv'\n",
    "locations = pd.read_csv(f\"{input_path}/{file_name}\")\n",
    "locations = locations[locations['flag']==num]\n",
    "\n",
    "thread_num = len(locations)\n",
    "print(f'Number of threads: {thread_num}')\n",
    "getlink_threads = [GetLinks() for _ in range(thread_num)]\n",
    "[getlink_thread.import_shuffle_df(f'{input_path}/{file_name}',num) for getlink_thread in getlink_threads]\n",
    "\n",
    "threadList = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for i in range(thread_num):\n",
    "        threadList.append(executor.submit(getlink_threads[i].get_condo_links, i+1))\n",
    "wait(threadList);\n",
    "\n",
    "getlink_threads[0].join_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81534f71",
   "metadata": {
    "papermill": {
     "duration": 0.005642,
     "end_time": "2023-07-27T13:21:11.425591",
     "exception": false,
     "start_time": "2023-07-27T13:21:11.419949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8478.560485,
   "end_time": "2023-07-27T13:21:12.468028",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-27T10:59:53.907543",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
