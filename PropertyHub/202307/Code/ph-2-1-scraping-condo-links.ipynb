{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7bd91b",
   "metadata": {
    "papermill": {
     "duration": 0.003745,
     "end_time": "2023-07-27T11:00:05.003874",
     "exception": false,
     "start_time": "2023-07-27T11:00:05.000129",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PropertyHub: 2.1 Scraping Condo Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9aaeb6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:00:05.014100Z",
     "iopub.status.busy": "2023-07-27T11:00:05.013650Z",
     "iopub.status.idle": "2023-07-27T11:00:05.291944Z",
     "shell.execute_reply": "2023-07-27T11:00:05.290154Z"
    },
    "papermill": {
     "duration": 0.287215,
     "end_time": "2023-07-27T11:00:05.295179",
     "exception": false,
     "start_time": "2023-07-27T11:00:05.007964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab9839c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:00:05.305960Z",
     "iopub.status.busy": "2023-07-27T11:00:05.305204Z",
     "iopub.status.idle": "2023-07-27T11:00:05.485551Z",
     "shell.execute_reply": "2023-07-27T11:00:05.484137Z"
    },
    "papermill": {
     "duration": 0.189264,
     "end_time": "2023-07-27T11:00:05.488425",
     "exception": false,
     "start_time": "2023-07-27T11:00:05.299161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GetLinks():\n",
    "    def __init__(self):\n",
    "        self.delay_time = 0.1\n",
    "        \n",
    "        self.rent_place_type = []\n",
    "        self.rent_place_name = []\n",
    "        self.rent_link_name = []\n",
    "        self.rent_condo_links = []\n",
    "        self.sale_place_type = []\n",
    "        self.sale_place_name = []\n",
    "        self.sale_link_name = []\n",
    "        self.sale_condo_links = []\n",
    "        \n",
    "        self.retries = 5\n",
    "        self.backoff = 1     # time-out = [0.5, 1, 2, 4, 8]\n",
    "        self.status_forcelist = [403, 500, 502, 503, 504]\n",
    "        self.timeout = (10, 10)\n",
    "        \n",
    "        # Initialize request session for retries and timeout\n",
    "        self.s = requests.Session()\n",
    "        retries = Retry(total=self.retries,\n",
    "                        backoff_factor=self.backoff,\n",
    "                        status_forcelist=self.status_forcelist)\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    def import_shuffle_df(self, path, num):\n",
    "        self.location = pd.read_csv(path)\n",
    "        self.location = self.location[self.location['flag']==num]\n",
    "        \n",
    "    def get_condo_links(self, order):\n",
    "        df_place_link_name = self.location.iloc[order-1,:]\n",
    "        place_link_name = df_place_link_name['link_name']\n",
    "        \n",
    "        print(f\"Thread {order}: getting {df_place_link_name['num_rent']+df_place_link_name['num_sale']} links ...\")\n",
    "        \n",
    "        # Get for-rent links\n",
    "        if df_place_link_name['num_rent'] != 0:\n",
    "            try:\n",
    "                place_link = f'https://propertyhub.in.th/en/condo-for-rent/{place_link_name}'\n",
    "                # find max page number\n",
    "                soup = BeautifulSoup(self.s.get(place_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                find_max_page = soup.find_all(\"ul\", {\"class\": \"sc-1p20b44-0 IoRRS\"})\n",
    "                try:\n",
    "                    max_page = int(find_max_page[0].find_all('li')[-2].a['aria-label'].split()[-1])\n",
    "                except:\n",
    "                    max_page = 1\n",
    "                page_links = [place_link + f'/{i+1}' if i!=0 else place_link for i in range(max_page)]\n",
    "                for page_link in page_links:\n",
    "                    soup = BeautifulSoup(self.s.get(page_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                    find_condo_links = soup.select(\"a[href*='en/listings/']\")\n",
    "                    for link in find_condo_links:\n",
    "                        condo_link = 'https://propertyhub.in.th' + link['href']\n",
    "                        self.rent_place_type.append(df_place_link_name['place_type'])\n",
    "                        self.rent_place_name.append(df_place_link_name['place_name'])\n",
    "                        self.rent_link_name.append(df_place_link_name['link_name'])\n",
    "                        self.rent_condo_links.append(condo_link)\n",
    "                time.sleep(self.delay_time)\n",
    "            except:\n",
    "                print(f\"There is an invalid link: {place_link}\")\n",
    "\n",
    "        # Get for-sale links\n",
    "        if df_place_link_name['num_sale'] != 0:\n",
    "            try:\n",
    "                place_link = f'https://propertyhub.in.th/en/condo-for-sale/{place_link_name}'\n",
    "                # find max page number\n",
    "                soup = BeautifulSoup(self.s.get(place_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                find_max_page = soup.find_all(\"ul\", {\"class\": \"sc-1p20b44-0 IoRRS\"})\n",
    "                try:\n",
    "                    max_page = int(find_max_page[0].find_all('li')[-2].a['aria-label'].split()[-1])\n",
    "                except:\n",
    "                    max_page = 1\n",
    "                page_links = [place_link + f'/{i+1}' if i!=0 else place_link for i in range(max_page)]\n",
    "                for page_link in page_links:\n",
    "                    soup = BeautifulSoup(self.s.get(page_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                    find_condo_links = soup.select(\"a[href*='en/listings/']\")\n",
    "                    for link in find_condo_links:\n",
    "                        condo_link = 'https://propertyhub.in.th' + link['href']\n",
    "                        self.sale_place_type.append(df_place_link_name['place_type'])\n",
    "                        self.sale_place_name.append(df_place_link_name['place_name'])\n",
    "                        self.sale_link_name.append(df_place_link_name['link_name'])\n",
    "                        self.sale_condo_links.append(condo_link)\n",
    "                time.sleep(self.delay_time)\n",
    "            except:\n",
    "                print(f\"There is an invalid link: {place_link}\")\n",
    "\n",
    "        # Convert to df and remove duplicates\n",
    "        self.df_rent_condo_links = pd.DataFrame(self.rent_place_type,columns=['place_type'])\n",
    "        self.df_rent_condo_links['place_name'] = self.rent_place_name\n",
    "        self.df_rent_condo_links['link_name'] = self.rent_link_name\n",
    "        self.df_rent_condo_links['condo_link'] = self.rent_condo_links\n",
    "        self.df_rent_condo_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "        self.df_sale_condo_links = pd.DataFrame(self.sale_place_type,columns=['place_type'])\n",
    "        self.df_sale_condo_links['place_name'] = self.sale_place_name\n",
    "        self.df_sale_condo_links['link_name'] = self.sale_link_name\n",
    "        self.df_sale_condo_links['condo_link'] = self.sale_condo_links\n",
    "        self.df_sale_condo_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "        # Export\n",
    "        self.df_rent_condo_links.to_csv(f\"df_rent_condo_links_{order}.csv\",index=False)\n",
    "        self.df_sale_condo_links.to_csv(f\"df_sale_condo_links_{order}.csv\",index=False)\n",
    "        \n",
    "        print(f\"Thread {order}: finished !!!\")\n",
    "            \n",
    "    def join_df(self):\n",
    "        output_rent_files = glob.glob(os.getcwd()+'/df_rent*.csv')\n",
    "        output_sale_files = glob.glob(os.getcwd()+'/df_sale*.csv')\n",
    "        outputs_rent = [pd.read_csv(output_rent_file) for output_rent_file in output_rent_files]\n",
    "        outputs_sale = [pd.read_csv(output_sale_file) for output_sale_file in output_sale_files]\n",
    "\n",
    "        # Combine outputs\n",
    "        output_rent_all = outputs_rent[0]\n",
    "        for i in range(len(outputs_rent)-1):\n",
    "            output_rent_all = pd.concat([output_rent_all,outputs_rent[i+1]], axis=0, ignore_index=True)\n",
    "        output_sale_all = outputs_sale[0]\n",
    "        for i in range(len(outputs_sale)-1):\n",
    "            output_sale_all = pd.concat([output_sale_all,outputs_sale[i+1]], axis=0, ignore_index=True)\n",
    "            \n",
    "        # Drop duplicates\n",
    "        output_rent_all.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "        output_sale_all.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'!!!!!! Finished Scraping Links !!!!!!')\n",
    "        print(f'for-rent data len: {len(output_rent_all)}')\n",
    "        print(f'for-sale data len: {len(output_sale_all)}')\n",
    "\n",
    "        # Export the combine result\n",
    "        output_rent_all.to_csv(f\"{datetime.now().strftime('%Y%m')}_rent_condo_links_1.csv\",index=False)\n",
    "        output_sale_all.to_csv(f\"{datetime.now().strftime('%Y%m')}_sale_condo_links_1.csv\",index=False)\n",
    "\n",
    "        # Delete all the unused files\n",
    "        [os.remove(output_rent_file) for output_rent_file in output_rent_files];\n",
    "        [os.remove(output_sale_file) for output_sale_file in output_sale_files];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae6cef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T11:00:05.496171Z",
     "iopub.status.busy": "2023-07-27T11:00:05.495744Z",
     "iopub.status.idle": "2023-07-27T13:11:42.191801Z",
     "shell.execute_reply": "2023-07-27T13:11:42.190477Z"
    },
    "papermill": {
     "duration": 7896.703391,
     "end_time": "2023-07-27T13:11:42.194926",
     "exception": false,
     "start_time": "2023-07-27T11:00:05.491535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of threads: 17\n",
      "Thread 1: getting 48442 links ...\n",
      "Thread 2: getting 47045 links ...\n",
      "Thread 3: getting 46887 links ...\n",
      "Thread 4: getting 45075 links ...\n",
      "Thread 5: getting 39936 links ...\n",
      "Thread 6: getting 33877 links ...\n",
      "Thread 7: getting 31341 links ...\n",
      "Thread 8: getting 25356 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/srinakharinwirot-university-prasarnmit-demonstration-school\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/srinakharinwirot-university-prasanmit-campus\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/wat-pathum-wanaram-school\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/university-of-the-thai-chamber-of-commerce\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-sale/srinakharinwirot-university-prasarnmit-demonstration-school\n",
      "Thread 1: finished !!!\n",
      "Thread 9: getting 22321 links ...\n",
      "Thread 8: finished !!!\n",
      "Thread 10: getting 20088 links ...\n",
      "Thread 7: finished !!!\n",
      "Thread 11: getting 19996 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/watthana\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/trinity-international-school\n",
      "Thread 3: finished !!!\n",
      "Thread 12: getting 17645 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/khlong-toei\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-sale/trinity-international-school\n",
      "Thread 5: finished !!!\n",
      "Thread 13: getting 17638 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/bangkok-university--3\n",
      "Thread 9: finished !!!\n",
      "Thread 14: getting 16841 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/ekamai-international-school\n",
      "Thread 6: finished !!!\n",
      "Thread 15: getting 16761 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/chulalongkorn-university\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/sai-namphueng-school\n",
      "Thread 10: finished !!!\n",
      "Thread 16: getting 16104 links ...\n",
      "Thread 12: finished !!!\n",
      "Thread 17: getting 16057 links ...\n",
      "There is an invalid link: https://propertyhub.in.th/en/condo-for-rent/bts-thong-lo-thong-lor\n",
      "Thread 2: finished !!!\n",
      "Thread 15: finished !!!\n",
      "Thread 4: finished !!!\n",
      "Thread 13: finished !!!\n",
      "Thread 11: finished !!!\n",
      "Thread 14: finished !!!\n",
      "Thread 16: finished !!!\n",
      "Thread 17: finished !!!\n",
      "!!!!!! Finished Scraping Links !!!!!!\n",
      "for-rent data len: 45612\n",
      "for-sale data len: 20814\n"
     ]
    }
   ],
   "source": [
    "num = 1\n",
    "input_path = '/kaggle/input/ph-1-getting-locations'\n",
    "file_name = 'locations.csv'\n",
    "locations = pd.read_csv(f\"{input_path}/{file_name}\")\n",
    "locations = locations[locations['flag']==num]\n",
    "\n",
    "thread_num = len(locations)\n",
    "print(f'Number of threads: {thread_num}')\n",
    "getlink_threads = [GetLinks() for _ in range(thread_num)]\n",
    "[getlink_thread.import_shuffle_df(f'{input_path}/{file_name}',num) for getlink_thread in getlink_threads]\n",
    "\n",
    "threadList = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for i in range(thread_num):\n",
    "        threadList.append(executor.submit(getlink_threads[i].get_condo_links, i+1))\n",
    "wait(threadList);\n",
    "\n",
    "getlink_threads[0].join_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a40635",
   "metadata": {
    "papermill": {
     "duration": 0.005529,
     "end_time": "2023-07-27T13:11:42.206360",
     "exception": false,
     "start_time": "2023-07-27T13:11:42.200831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7915.406239,
   "end_time": "2023-07-27T13:11:43.458100",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-27T10:59:48.051861",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
