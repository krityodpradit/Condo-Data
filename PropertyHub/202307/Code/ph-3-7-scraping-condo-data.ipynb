{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ea6fa1",
   "metadata": {
    "papermill": {
     "duration": 0.00365,
     "end_time": "2023-07-27T15:54:28.410757",
     "exception": false,
     "start_time": "2023-07-27T15:54:28.407107",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PropertyHub: 3.6 Scraping Condo Data (for-sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4addafcf",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-27T15:54:28.418925Z",
     "iopub.status.busy": "2023-07-27T15:54:28.418410Z",
     "iopub.status.idle": "2023-07-27T15:54:28.676439Z",
     "shell.execute_reply": "2023-07-27T15:54:28.675162Z"
    },
    "papermill": {
     "duration": 0.265944,
     "end_time": "2023-07-27T15:54:28.679941",
     "exception": false,
     "start_time": "2023-07-27T15:54:28.413997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a0339a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T15:54:28.690659Z",
     "iopub.status.busy": "2023-07-27T15:54:28.689763Z",
     "iopub.status.idle": "2023-07-27T15:54:28.930602Z",
     "shell.execute_reply": "2023-07-27T15:54:28.929134Z"
    },
    "papermill": {
     "duration": 0.250856,
     "end_time": "2023-07-27T15:54:28.934445",
     "exception": false,
     "start_time": "2023-07-27T15:54:28.683589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScrapeData():\n",
    "    def __init__(self):\n",
    "        self.delay_time = 0     # **** Please set this variable > 1 not to overload the server ****\n",
    "        self.condo_level_data = []\n",
    "        self.parse_count = 0\n",
    "        self.error_index = []\n",
    "        self.invalid_link_count = 0\n",
    "        self.retries = 5\n",
    "        self.backoff = 1     # time-out = [0.5, 1, 2, 4, 8]\n",
    "        self.status_forcelist = [403, 500, 502, 503, 504]\n",
    "        self.timeout = (10, 10)\n",
    "        \n",
    "        # Initialize request session for retries and timeout\n",
    "        self.s = requests.Session()\n",
    "        retries = Retry(total=self.retries,\n",
    "                        backoff_factor=self.backoff,\n",
    "                        status_forcelist=self.status_forcelist)\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "        \n",
    "    def parse_data(self):\n",
    "        for condo_link in self.condo_links:\n",
    "            try:\n",
    "                page = self.s.get(condo_link, timeout=self.timeout)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                # Parse condo details\n",
    "                post_name = soup.select(\"h1[class*='sc-14haut3-0 kiSLkD']\")[0].text.strip()\n",
    "                last_update_date = soup.select(\"div[class*='sc-ogfj7g-14 coaZDA']\")[0].text.strip().split()[-2]\n",
    "                last_update_time = soup.select(\"div[class*='sc-ogfj7g-14 coaZDA']\")[0].text.strip().split()[-1]\n",
    "                try:\n",
    "                    poster_name = soup.select(\"li[class*='sc-ves8oa-9 hCPUWp']\")[0].div.a.text\n",
    "                except:\n",
    "                    try:\n",
    "                        poster_name = soup.select(\"li[class*='sc-ves8oa-9 hCPUWp']\")[0].div.p.text\n",
    "                    except:\n",
    "                        poster_name = ''\n",
    "                poster_status = soup.select(\"li[class*='sc-ves8oa-9 hCPUWp']\")[0].div.div.text\n",
    "                # view_count = soup.select(\"div[class*='sc-ves8oa-0 hWLLWW']\")[0].text.strip().split()[-1]        # Cannot get view count as the page needs to load up\n",
    "                \n",
    "                if self.rent_flag == 1:\n",
    "                    price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[-1].text\n",
    "                    if price_unit.lower().strip() == 'please contact':\n",
    "                        try:\n",
    "                            price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[1].text.split('THB')[0].replace(',',''))\n",
    "                            price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[2].text.strip()\n",
    "                        except:\n",
    "                            price = 'please'\n",
    "                            price_unit = 'contact'\n",
    "                    else:\n",
    "                        price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[-2].text.split('THB')[0].replace(',',''))\n",
    "\n",
    "                    deposit = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[0].find_all('p')[-1].text.strip().split()[0]\n",
    "                    try:\n",
    "                        deposit_unit = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[0].find_all('p')[-1].text.strip().split()[1]\n",
    "                    except:\n",
    "                        deposit_unit = \"\"\n",
    "                    advance_payment = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[1].find_all('p')[-1].text.strip().split()[0]\n",
    "                    try:\n",
    "                        advance_payment_unit = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[1].find_all('p')[-1].text.strip().split()[1]\n",
    "                    except:\n",
    "                        advance_payment_unit = \"\"\n",
    "                else:\n",
    "                    price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[-1].text\n",
    "                    if price_unit.lower().strip() == 'please contact':\n",
    "                        try:\n",
    "                            price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[1].text.split('THB')[0].replace(',',''))\n",
    "                            price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[2].text.strip()\n",
    "                        except:\n",
    "                            price = 'please'\n",
    "                            price_unit = 'contact'\n",
    "                    else:\n",
    "                        price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[-2].text.split('THB')[0].replace(',',''))\n",
    "                    \n",
    "                    deposit = ''\n",
    "                    deposit_unit = ''\n",
    "                    advance_payment = ''\n",
    "                    advance_payment_unit = ''\n",
    "                    \n",
    "                room_info_header = [room_info.p.text.strip(':') for room_info in soup.select(\"li[class*='sc-s9r052-1 bLavUw']\")]\n",
    "                room_info_value = [room_info.span.text.strip() for room_info in soup.select(\"li[class*='sc-s9r052-1 bLavUw']\")]\n",
    "                find_room_description = soup.select(\"div[class*='sc-ves8oa-21 bBnKvP']\")\n",
    "                if len(find_room_description) == 0:\n",
    "                    room_description = []\n",
    "                else:\n",
    "                    room_description = find_room_description[0].text.strip()\n",
    "\n",
    "                room_amenities_have = [amen.text.strip() for amen in soup.select(\"div[class*='sc-1qj7qf1-1 czrvpe']\")[0].find_all('span')]\n",
    "                room_amenities_not_have = [amen.text.strip() for amen in soup.select(\"div[class*='sc-1qj7qf1-1 czrvpe']\")[0].find_all('strike')]\n",
    "\n",
    "                # Parse project details\n",
    "                project_details_attributes = [header.text.strip() for header in soup.select(\"table[class*='sc-7l0zor-1 jEPVvF']\")[0].find_all('th')]\n",
    "                project_details_values = []\n",
    "                for value in soup.select(\"table[class*='sc-7l0zor-1 jEPVvF']\")[0].find_all('td'):\n",
    "                    if len(value.find_all('li')) <= 1:\n",
    "                        project_details_values.append(value.text.strip())\n",
    "                    else:\n",
    "                        value_list = [v.text.strip() for v in value.find_all('li')]\n",
    "                        project_details_values.append(value_list)\n",
    "                facilities_have = [facil.text.strip() for facil in soup.select(\"div[class*='sc-vxzykp-0 dTLeQV sc-ogfj7g-18 sc-iv2rdv-17 bLEjbt citTje']\")[0].find_all('span')]\n",
    "                facilities_not_have = [facil.text.strip() for facil in soup.select(\"div[class*='sc-vxzykp-0 dTLeQV sc-ogfj7g-18 sc-iv2rdv-17 bLEjbt citTje']\")[0].find_all('strike')]\n",
    "\n",
    "                # Parse properties in nearby area\n",
    "                nearby_property_type = []\n",
    "                nearby_property_name = []\n",
    "                nearby_property_distance = []\n",
    "\n",
    "                blocks = soup.select(\"div[class*='sc-vxzykp-0 dNnjli sc-nnw194-1 bnkbhT']\")\n",
    "                blocks.extend(soup.select(\"div[class*='sc-vxzykp-0 dTLeQV sc-nnw194-1 bnkbhT']\"))\n",
    "\n",
    "                for block in blocks:\n",
    "                    for i,sub_block in enumerate(block.select(\"div[class*='row sc-nnw194-2 gAWLJy']\")):\n",
    "                        for prop in sub_block.select(\"a[class*='zoneTypeStyle']\"):\n",
    "                            nearby_property_type.append(block.find_all('h3')[i].text.strip())\n",
    "                            nearby_property_name.append(prop.text.strip().replace('Condo ',''))\n",
    "                            if len(sub_block.find_all('span'))==0:\n",
    "                                nearby_property_distance.append('')\n",
    "                            else:\n",
    "                                nearby_property_distance.append(sub_block.find_all('span')[0].text.strip())\n",
    "\n",
    "                self.condo_level_data.append([self.rent_flag,\n",
    "                                            condo_link,\n",
    "                                            post_name,\n",
    "                                            last_update_date,\n",
    "                                            last_update_time,\n",
    "                                            poster_name,\n",
    "                                            poster_status,\n",
    "                                            price,\n",
    "                                            price_unit,\n",
    "                                            deposit,\n",
    "                                            deposit_unit,\n",
    "                                            advance_payment,\n",
    "                                            advance_payment_unit,\n",
    "                                            room_info_header,\n",
    "                                            room_info_value,\n",
    "                                            room_description,\n",
    "                                            room_amenities_have,\n",
    "                                            room_amenities_not_have,\n",
    "                                            project_details_attributes,\n",
    "                                            project_details_values,\n",
    "                                            facilities_have,\n",
    "                                            facilities_not_have,\n",
    "                                            nearby_property_type,\n",
    "                                            nearby_property_name,\n",
    "                                            nearby_property_distance])\n",
    "\n",
    "                self.parse_count += 1\n",
    "                time.sleep(self.delay_time)\n",
    "                \n",
    "            except:\n",
    "                self.condo_level_data.append([])\n",
    "                if (page.url == 'https://propertyhub.in.th/en') or (len(soup.select(\"div[class*='sc-1552ugy-1 sc-1552ugy-5 kvqbdg eugViW']\")) != 0):\n",
    "                    self.invalid_link_count += 1 \n",
    "                    self.parse_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f'Error at {condo_link}')\n",
    "                    self.error_index.append(self.parse_count)\n",
    "                    self.parse_count += 1\n",
    "                    continue\n",
    "                \n",
    "    \n",
    "    def export_results(self):\n",
    "        condo_data = pd.DataFrame(self.condo_level_data)\n",
    "        col_names = ['rent_flag',\n",
    "                    'condo_link',\n",
    "                    'post_name',\n",
    "                    'last_update_date',\n",
    "                    'last_update_time',\n",
    "                    'poster_name',\n",
    "                    'poster_status',\n",
    "                    'price',\n",
    "                    'price_unit',\n",
    "                    'deposit',\n",
    "                    'deposit_unit',\n",
    "                    'advance_payment',\n",
    "                    'advance_payment_unit',\n",
    "                    'room_info_header',\n",
    "                    'room_info_value',\n",
    "                    'room_description',\n",
    "                    'room_amenities_have',\n",
    "                    'room_amenities_not_have',\n",
    "                    'project_details_attributes',\n",
    "                    'project_details_values',\n",
    "                    'facilities_have',\n",
    "                    'facilities_not_have',\n",
    "                    'nearby_property_type',\n",
    "                    'nearby_property_name',\n",
    "                    'nearby_property_distance']\n",
    "        \n",
    "        condo_data.columns = col_names\n",
    "        condo_data.to_csv(f\"condo_data_{self.order}.csv\",index=False)\n",
    "    \n",
    "    def main(self, df_links, order, rent_flag=1):\n",
    "        self.condo_links = df_links.iloc[:,3]\n",
    "        self.order = order\n",
    "        self.rent_flag = rent_flag   # 1 for-rent and 0 for-sale\n",
    "        \n",
    "        print(f'Thread {self.order}: Scraping for {len(self.condo_links)} links ...')\n",
    "        \n",
    "        self.parse_data()\n",
    "        self.export_results()\n",
    "        \n",
    "        if len(self.error_index) > 0:\n",
    "            print(f'Thread {self.order}: !!! Scraping completed ({len(self.error_index)} errors, {self.invalid_link_count} invalid links)')\n",
    "        else:\n",
    "            print(f'Thread {self.order}: !!! Scraping completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39d8d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T15:54:28.945075Z",
     "iopub.status.busy": "2023-07-27T15:54:28.944557Z",
     "iopub.status.idle": "2023-07-27T15:54:29.664859Z",
     "shell.execute_reply": "2023-07-27T15:54:29.663554Z"
    },
    "papermill": {
     "duration": 0.728725,
     "end_time": "2023-07-27T15:54:29.668255",
     "exception": false,
     "start_time": "2023-07-27T15:54:28.939530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd batch: for-sale data len: 18918\n"
     ]
    }
   ],
   "source": [
    "# Once scraping condo links notebook is done, upload the generated output files to this notebook and update the input path\n",
    "path_1 = '/kaggle/input/ph-2-1-scraping-condo-links'\n",
    "df_sale_links_1 = pd.read_csv(f\"{path_1}/{datetime.now().strftime('%Y%m')}_sale_condo_links_1.csv\")\n",
    "\n",
    "path_2 = '/kaggle/input/ph-2-2-scraping-condo-links'\n",
    "df_sale_links_2 = pd.read_csv(f\"{path_2}/{datetime.now().strftime('%Y%m')}_sale_condo_links_2.csv\")\n",
    "\n",
    "path_3 = '/kaggle/input/ph-2-3-scraping-condo-links'\n",
    "df_sale_links_3 = pd.read_csv(f\"{path_3}/{datetime.now().strftime('%Y%m')}_sale_condo_links_3.csv\")\n",
    "\n",
    "path_4 = '/kaggle/input/ph-2-4-scraping-condo-links'\n",
    "df_sale_links_4 = pd.read_csv(f\"{path_4}/{datetime.now().strftime('%Y%m')}_sale_condo_links_4.csv\")\n",
    "\n",
    "df_sale_links = pd.concat([df_sale_links_1,df_sale_links_2,df_sale_links_3,df_sale_links_4], axis=0, ignore_index=True)\n",
    "df_sale_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "# Dividing into groups\n",
    "sale_len = round(len(df_sale_links)/2)\n",
    "print(f'2nd batch: for-sale data len: {sale_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3498126b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T15:54:29.677723Z",
     "iopub.status.busy": "2023-07-27T15:54:29.677265Z",
     "iopub.status.idle": "2023-07-27T21:19:12.164736Z",
     "shell.execute_reply": "2023-07-27T21:19:12.163260Z"
    },
    "papermill": {
     "duration": 19482.49966,
     "end_time": "2023-07-27T21:19:12.171572",
     "exception": false,
     "start_time": "2023-07-27T15:54:29.671912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 17: Scraping for 1182 links ...\n",
      "Thread 18: Scraping for 1182 links ...\n",
      "Thread 19: Scraping for 1183 links ...\n",
      "Thread 20: Scraping for 1182 links ...\n",
      "Thread 21: Scraping for 1182 links ...\n",
      "Thread 22: Scraping for 1183 links ...\n",
      "Thread 23: Scraping for 1182 links ...\n",
      "Thread 24: Scraping for 1182 links ...\n",
      "Thread 23: !!! Scraping completed\n",
      "Thread 25: Scraping for 1183 links ...\n",
      "Thread 24: !!! Scraping completed\n",
      "Thread 26: Scraping for 1182 links ...\n",
      "Error at https://propertyhub.in.th/en/listings/for-sell-ready-to-move-in-regent-home-27-bangson---3835968\n",
      "Thread 22: !!! Scraping completed\n",
      "Thread 27: Scraping for 1182 links ...\n",
      "Thread 18: !!! Scraping completed\n",
      "Thread 28: Scraping for 1183 links ...\n",
      "Thread 21: !!! Scraping completed\n",
      "Thread 29: Scraping for 1182 links ...\n",
      "Thread 20: !!! Scraping completed\n",
      "Thread 30: Scraping for 1182 links ...\n",
      "Thread 19: !!! Scraping completed (1 errors, 2 invalid links)\n",
      "Thread 31: Scraping for 1183 links ...\n",
      "Thread 17: !!! Scraping completed\n",
      "Thread 32: Scraping for 1182 links ...\n",
      "Error at https://propertyhub.in.th/en/listings/ae66024-sell-the-iris-rama-9-srinakarin-1-bedroom-45-5-sqm-high-floor-pool-view-brand-new-furniture---3731894\n",
      "Error at https://propertyhub.in.th/en/listings/condo-for-sale-lumpini-ville-on-nut-phatthanakan-lumpini-ville-onnut-pattanakarn--b4e70a0b---3801584\n",
      "Error at https://propertyhub.in.th/en/listings/oneplus-jedyod1-condo-for-sale-moutain-view-near-chiangmai-university--ab1c0941---3600540\n",
      "Error at https://propertyhub.in.th/en/listings/for-sale-dcondo-campus-resort-rangsit-phase-1-c-building-6th-floor-1-7-mb---3184031\n",
      "Error at https://propertyhub.in.th/en/listings/city-garden-condo-for-sale-in-pattaya---3695667\n",
      "Error at https://propertyhub.in.th/en/listings/cs298-for-sale-lumpini-park-riverside-rama-3-brt-wat-dok-mai-064-665-4666---3716535\n",
      "Error at https://propertyhub.in.th/en/listings/studio-pattaya-plaza-condotel-for-sale--b01209c9---3716357\n",
      "Error at https://propertyhub.in.th/en/listings/urgently-starview-rama-3-for-sale-11-99m-with-fully-furnished---3760087\n",
      "Error at https://propertyhub.in.th/en/listings/condo-for-sale-the-base-height-mittraphap-khonkaen---3741417\n",
      "Error at https://propertyhub.in.th/en/listings/property-id-220cs-chom-doi-condo-square-building-1bed1bath35m2-near-chiang-mai-university---3761304\n",
      "Error at https://propertyhub.in.th/en/listings/sale-the-parkland-lite-sukhumvit-paknam-owner--ae0409f1---3689601\n",
      "Error at https://propertyhub.in.th/en/listings/novel-2-br-condo-at-grand-avenue-pattaya-close-to-pattaya-klang-id-1003160--a4f80923---3785066\n",
      "Error at https://propertyhub.in.th/en/listings/sell-baan-suan-thon-condominium-soi-bhuddhabucha-47-size-56-sq-m-2-bedrooms-1-bathroom-full-furnished---3756247\n",
      "Error at https://propertyhub.in.th/en/listings/condo-for-sale-d-condo-campus-resort-bangna-1-bedroom-1st-floor-building-c-near-abac-bangna-line-id-501farwc---3838317\n",
      "Error at https://propertyhub.in.th/en/listings/modern-2-br-condo-at-the-base-central-pattaya-id-1002892--a7a908c9---3785075\n",
      "Error at https://propertyhub.in.th/en/listings/property-id244cs-serano-condo-1bed-1bath-26m2-nearbyairport-cnx---3764062\n",
      "Error at https://propertyhub.in.th/en/listings/condo-for-sale-lumpini-park-riverside-rama-3-chao-phraya-riverside-beautiful-room-fully-built-in-furniture-very-cheap-price-line-id-ppagent---2743711\n",
      "Thread 29: !!! Scraping completed (2 errors, 2 invalid links)\n",
      "Thread 30: !!! Scraping completed (1 errors, 33 invalid links)\n",
      "Thread 25: !!! Scraping completed (1 errors, 2 invalid links)\n",
      "Thread 28: !!! Scraping completed (1 errors, 2 invalid links)\n",
      "Thread 27: !!! Scraping completed (3 errors, 28 invalid links)\n",
      "Thread 31: !!! Scraping completed (4 errors, 4 invalid links)\n",
      "Thread 26: !!! Scraping completed (5 errors, 6 invalid links)\n",
      "Thread 32: !!! Scraping completed\n"
     ]
    }
   ],
   "source": [
    "# Scraping for-sale condo data\n",
    "sale_group_num = 32\n",
    "sale_links = []\n",
    "for i in range(sale_group_num):\n",
    "    start = round(i*len(df_sale_links)/sale_group_num)\n",
    "    end = round((i+1)*len(df_sale_links)/sale_group_num)\n",
    "    sale_links.append(df_sale_links.iloc[start:end,:])\n",
    "\n",
    "sale_thread_num = 16\n",
    "sale_scrape_threads = [ScrapeData() for _ in range(sale_thread_num)]\n",
    "sale_threadList = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for i in range(sale_thread_num):\n",
    "        sale_threadList.append(executor.submit(sale_scrape_threads[i].main, sale_links[i+16], i+1+16, 0))\n",
    "wait(sale_threadList);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53662881",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-27T21:19:12.187709Z",
     "iopub.status.busy": "2023-07-27T21:19:12.187245Z",
     "iopub.status.idle": "2023-07-27T21:19:16.238490Z",
     "shell.execute_reply": "2023-07-27T21:19:16.237490Z"
    },
    "papermill": {
     "duration": 4.062904,
     "end_time": "2023-07-27T21:19:16.241292",
     "exception": false,
     "start_time": "2023-07-27T21:19:12.178388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total output length: 18800\n"
     ]
    }
   ],
   "source": [
    "# Read all files in the output folder\n",
    "output_files = glob.glob(f\"{os.getcwd()}/condo_data*.csv\")\n",
    "outputs = [pd.read_csv(f'{output_file}') for output_file in output_files]\n",
    "\n",
    "# Combine outputs\n",
    "output_all = outputs[0]\n",
    "for i in range(len(outputs)-1):\n",
    "    output_all = pd.concat([output_all,outputs[i+1]], axis=0, ignore_index=True)\n",
    "\n",
    "# Drop empty rows resulting from errors from scraping\n",
    "output_all.dropna(subset=['post_name'], axis=0, inplace=True)\n",
    "output_all.reset_index(inplace=True, drop=True)\n",
    "output = output_all\n",
    "\n",
    "# Export the combine result\n",
    "output.to_csv(f\"{datetime.now().strftime('%Y%m')}_condo_data_7.csv\",index=False)\n",
    "\n",
    "# Delete all the unused files\n",
    "[os.remove(output_file) for output_file in output_files];\n",
    "\n",
    "# Show results\n",
    "print(f'Total output length: {len(output)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19505.046968,
   "end_time": "2023-07-27T21:19:17.718342",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-27T15:54:12.671374",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
