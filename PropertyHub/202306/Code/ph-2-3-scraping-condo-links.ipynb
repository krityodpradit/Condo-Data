{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1abd4b",
   "metadata": {
    "papermill": {
     "duration": 0.00308,
     "end_time": "2023-06-30T09:26:29.111875",
     "exception": false,
     "start_time": "2023-06-30T09:26:29.108795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PropertyHub: 2.3 Scraping Condo Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf565a0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-30T09:26:29.118960Z",
     "iopub.status.busy": "2023-06-30T09:26:29.118510Z",
     "iopub.status.idle": "2023-06-30T09:26:29.328129Z",
     "shell.execute_reply": "2023-06-30T09:26:29.326703Z"
    },
    "papermill": {
     "duration": 0.216496,
     "end_time": "2023-06-30T09:26:29.331102",
     "exception": false,
     "start_time": "2023-06-30T09:26:29.114606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa191be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T09:26:29.339079Z",
     "iopub.status.busy": "2023-06-30T09:26:29.337976Z",
     "iopub.status.idle": "2023-06-30T09:26:29.504318Z",
     "shell.execute_reply": "2023-06-30T09:26:29.502921Z"
    },
    "papermill": {
     "duration": 0.173383,
     "end_time": "2023-06-30T09:26:29.507113",
     "exception": false,
     "start_time": "2023-06-30T09:26:29.333730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GetLinks():\n",
    "    def __init__(self):\n",
    "        self.delay_time = 0.1\n",
    "        \n",
    "        self.rent_place_type = []\n",
    "        self.rent_place_name = []\n",
    "        self.rent_link_name = []\n",
    "        self.rent_condo_links = []\n",
    "        self.sale_place_type = []\n",
    "        self.sale_place_name = []\n",
    "        self.sale_link_name = []\n",
    "        self.sale_condo_links = []\n",
    "        \n",
    "        self.retries = 5\n",
    "        self.backoff = 1     # time-out = [0.5, 1, 2, 4, 8]\n",
    "        self.status_forcelist = [403, 500, 502, 503, 504]\n",
    "        self.timeout = (10, 10)\n",
    "        \n",
    "        # Initialize request session for retries and timeout\n",
    "        self.s = requests.Session()\n",
    "        retries = Retry(total=self.retries,\n",
    "                        backoff_factor=self.backoff,\n",
    "                        status_forcelist=self.status_forcelist)\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    def import_shuffle_df(self, path, num):\n",
    "        self.location = pd.read_csv(path)\n",
    "        self.location = self.location[self.location['flag']==num]\n",
    "        \n",
    "    def get_condo_links(self, order):\n",
    "        df_place_link_name = self.location.iloc[order-1,:]\n",
    "        place_link_name = df_place_link_name['link_name']\n",
    "        \n",
    "        print(f\"Thread {order}: getting {df_place_link_name['num_rent']+df_place_link_name['num_sale']} links ...\")\n",
    "        \n",
    "        # Get for-rent links\n",
    "        if df_place_link_name['num_rent'] != 0:\n",
    "            try:\n",
    "                place_link = f'https://propertyhub.in.th/en/condo-for-rent/{place_link_name}'\n",
    "                # find max page number\n",
    "                soup = BeautifulSoup(self.s.get(place_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                find_max_page = soup.find_all(\"ul\", {\"class\": \"sc-1p20b44-0 IoRRS\"})\n",
    "                try:\n",
    "                    max_page = int(find_max_page[0].find_all('li')[-2].a['aria-label'].split()[-1])\n",
    "                except:\n",
    "                    max_page = 1\n",
    "                page_links = [place_link + f'/{i+1}' if i!=0 else place_link for i in range(max_page)]\n",
    "                for page_link in page_links:\n",
    "                    soup = BeautifulSoup(self.s.get(page_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                    find_condo_links = soup.select(\"a[href*='en/listings/']\")\n",
    "                    for link in find_condo_links:\n",
    "                        condo_link = 'https://propertyhub.in.th' + link['href']\n",
    "                        self.rent_place_type.append(df_place_link_name['place_type'])\n",
    "                        self.rent_place_name.append(df_place_link_name['place_name'])\n",
    "                        self.rent_link_name.append(df_place_link_name['link_name'])\n",
    "                        self.rent_condo_links.append(condo_link)\n",
    "                time.sleep(self.delay_time)\n",
    "            except:\n",
    "                print(f\"There is an invalid link: {place_link}\")\n",
    "\n",
    "        # Get for-sale links\n",
    "        if df_place_link_name['num_sale'] != 0:\n",
    "            try:\n",
    "                place_link = f'https://propertyhub.in.th/en/condo-for-sale/{place_link_name}'\n",
    "                # find max page number\n",
    "                soup = BeautifulSoup(self.s.get(place_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                find_max_page = soup.find_all(\"ul\", {\"class\": \"sc-1p20b44-0 IoRRS\"})\n",
    "                try:\n",
    "                    max_page = int(find_max_page[0].find_all('li')[-2].a['aria-label'].split()[-1])\n",
    "                except:\n",
    "                    max_page = 1\n",
    "                page_links = [place_link + f'/{i+1}' if i!=0 else place_link for i in range(max_page)]\n",
    "                for page_link in page_links:\n",
    "                    soup = BeautifulSoup(self.s.get(page_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                    find_condo_links = soup.select(\"a[href*='en/listings/']\")\n",
    "                    for link in find_condo_links:\n",
    "                        condo_link = 'https://propertyhub.in.th' + link['href']\n",
    "                        self.sale_place_type.append(df_place_link_name['place_type'])\n",
    "                        self.sale_place_name.append(df_place_link_name['place_name'])\n",
    "                        self.sale_link_name.append(df_place_link_name['link_name'])\n",
    "                        self.sale_condo_links.append(condo_link)\n",
    "                time.sleep(self.delay_time)\n",
    "            except:\n",
    "                print(f\"There is an invalid link: {place_link}\")\n",
    "\n",
    "        # Convert to df and remove duplicates\n",
    "        self.df_rent_condo_links = pd.DataFrame(self.rent_place_type,columns=['place_type'])\n",
    "        self.df_rent_condo_links['place_name'] = self.rent_place_name\n",
    "        self.df_rent_condo_links['link_name'] = self.rent_link_name\n",
    "        self.df_rent_condo_links['condo_link'] = self.rent_condo_links\n",
    "        self.df_rent_condo_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "        self.df_sale_condo_links = pd.DataFrame(self.sale_place_type,columns=['place_type'])\n",
    "        self.df_sale_condo_links['place_name'] = self.sale_place_name\n",
    "        self.df_sale_condo_links['link_name'] = self.sale_link_name\n",
    "        self.df_sale_condo_links['condo_link'] = self.sale_condo_links\n",
    "        self.df_sale_condo_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "        # Export\n",
    "        self.df_rent_condo_links.to_csv(f\"df_rent_condo_links_{order}.csv\",index=False)\n",
    "        self.df_sale_condo_links.to_csv(f\"df_sale_condo_links_{order}.csv\",index=False)\n",
    "        \n",
    "        print(f\"Thread {order}: finished !!!\")\n",
    "            \n",
    "    def join_df(self):\n",
    "        output_rent_files = glob.glob(os.getcwd()+'/df_rent*.csv')\n",
    "        output_sale_files = glob.glob(os.getcwd()+'/df_sale*.csv')\n",
    "        outputs_rent = [pd.read_csv(output_rent_file) for output_rent_file in output_rent_files]\n",
    "        outputs_sale = [pd.read_csv(output_sale_file) for output_sale_file in output_sale_files]\n",
    "\n",
    "        # Combine outputs\n",
    "        output_rent_all = outputs_rent[0]\n",
    "        for i in range(len(outputs_rent)-1):\n",
    "            output_rent_all = pd.concat([output_rent_all,outputs_rent[i+1]], axis=0, ignore_index=True)\n",
    "        output_sale_all = outputs_sale[0]\n",
    "        for i in range(len(outputs_sale)-1):\n",
    "            output_sale_all = pd.concat([output_sale_all,outputs_sale[i+1]], axis=0, ignore_index=True)\n",
    "            \n",
    "        # Drop duplicates\n",
    "        output_rent_all.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "        output_sale_all.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'!!!!!! Finished Scraping Links !!!!!!')\n",
    "        print(f'for-rent data len: {len(output_rent_all)}')\n",
    "        print(f'for-sale data len: {len(output_sale_all)}')\n",
    "\n",
    "        # Export the combine result\n",
    "        output_rent_all.to_csv(f\"{datetime.now().strftime('%Y%m')}_rent_condo_links_3.csv\",index=False)\n",
    "        output_sale_all.to_csv(f\"{datetime.now().strftime('%Y%m')}_sale_condo_links_3.csv\",index=False)\n",
    "\n",
    "        # Delete all the unused files\n",
    "        [os.remove(output_rent_file) for output_rent_file in output_rent_files];\n",
    "        [os.remove(output_sale_file) for output_sale_file in output_sale_files];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a47250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T09:26:29.515904Z",
     "iopub.status.busy": "2023-06-30T09:26:29.515048Z",
     "iopub.status.idle": "2023-06-30T11:35:20.383952Z",
     "shell.execute_reply": "2023-06-30T11:35:20.382720Z"
    },
    "papermill": {
     "duration": 7730.876777,
     "end_time": "2023-06-30T11:35:20.386905",
     "exception": false,
     "start_time": "2023-06-30T09:26:29.510128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of threads: 70\n",
      "Thread 1: getting 11602 links ...\n",
      "Thread 2: getting 11463 links ...\n",
      "Thread 3: getting 11201 links ...\n",
      "Thread 4: getting 10778 links ...\n",
      "Thread 5: getting 10733 links ...\n",
      "Thread 6: getting 10018 links ...\n",
      "Thread 7: getting 9616 links ...\n",
      "Thread 8: getting 9610 links ...\n",
      "Thread 3: finished !!!\n",
      "Thread 9: getting 9505 links ...\n",
      "Thread 6: finished !!!\n",
      "Thread 10: getting 9245 links ...\n",
      "Thread 7: finished !!!\n",
      "Thread 11: getting 8997 links ...\n",
      "Thread 5: finished !!!\n",
      "Thread 12: getting 8953 links ...\n",
      "Thread 4: finished !!!\n",
      "Thread 13: getting 8953 links ...\n",
      "Thread 8: finished !!!\n",
      "Thread 14: getting 8944 links ...\n",
      "Thread 2: finished !!!\n",
      "Thread 15: getting 8837 links ...\n",
      "Thread 1: finished !!!\n",
      "Thread 16: getting 8607 links ...\n",
      "Thread 9: finished !!!\n",
      "Thread 17: getting 8571 links ...\n",
      "Thread 12: finished !!!\n",
      "Thread 18: getting 8551 links ...\n",
      "Thread 16: finished !!!\n",
      "Thread 19: getting 8232 links ...\n",
      "Thread 15: finished !!!\n",
      "Thread 20: getting 8114 links ...\n",
      "Thread 11: finished !!!\n",
      "Thread 21: getting 7991 links ...\n",
      "Thread 14: finished !!!\n",
      "Thread 22: getting 7838 links ...\n",
      "Thread 10: finished !!!\n",
      "Thread 23: getting 7744 links ...\n",
      "Thread 13: finished !!!\n",
      "Thread 24: getting 7623 links ...\n",
      "Thread 18: finished !!!\n",
      "Thread 25: getting 7587 links ...\n",
      "Thread 22: finished !!!\n",
      "Thread 26: getting 7230 links ...\n",
      "Thread 17: finished !!!\n",
      "Thread 27: getting 7191 links ...\n",
      "Thread 23: finished !!!\n",
      "Thread 28: getting 7164 links ...\n",
      "Thread 19: finished !!!\n",
      "Thread 29: getting 7142 links ...\n",
      "Thread 24: finished !!!\n",
      "Thread 30: getting 6867 links ...\n",
      "Thread 20: finished !!!\n",
      "Thread 31: getting 6814 links ...\n",
      "Thread 21: finished !!!\n",
      "Thread 32: getting 6753 links ...\n",
      "Thread 27: finished !!!\n",
      "Thread 33: getting 6703 links ...\n",
      "Thread 29: finished !!!\n",
      "Thread 34: getting 6615 links ...\n",
      "Thread 26: finished !!!\n",
      "Thread 35: getting 6322 links ...\n",
      "Thread 28: finished !!!\n",
      "Thread 36: getting 6304 links ...\n",
      "Thread 25: finished !!!\n",
      "Thread 37: getting 6187 links ...\n",
      "Thread 31: finished !!!\n",
      "Thread 38: getting 6143 links ...\n",
      "Thread 30: finished !!!\n",
      "Thread 39: getting 6079 links ...\n",
      "Thread 32: finished !!!\n",
      "Thread 40: getting 6057 links ...\n",
      "Thread 35: finished !!!\n",
      "Thread 41: getting 5997 links ...\n",
      "Thread 34: finished !!!\n",
      "Thread 42: getting 5935 links ...\n",
      "Thread 36: finished !!!\n",
      "Thread 43: getting 5909 links ...\n",
      "Thread 33: finished !!!\n",
      "Thread 44: getting 5885 links ...\n",
      "Thread 39: finished !!!\n",
      "Thread 45: getting 5777 links ...\n",
      "Thread 38: finished !!!\n",
      "Thread 46: getting 5738 links ...\n",
      "Thread 37: finished !!!\n",
      "Thread 47: getting 5733 links ...\n",
      "Thread 40: finished !!!\n",
      "Thread 48: getting 5713 links ...\n",
      "Thread 42: finished !!!\n",
      "Thread 49: getting 5710 links ...\n",
      "Thread 41: finished !!!\n",
      "Thread 50: getting 5695 links ...\n",
      "Thread 43: finished !!!\n",
      "Thread 51: getting 5618 links ...\n",
      "Thread 46: finished !!!\n",
      "Thread 52: getting 5608 links ...\n",
      "Thread 44: finished !!!\n",
      "Thread 53: getting 5563 links ...\n",
      "Thread 45: finished !!!\n",
      "Thread 54: getting 5423 links ...\n",
      "Thread 47: finished !!!\n",
      "Thread 55: getting 5268 links ...\n",
      "Thread 50: finished !!!\n",
      "Thread 56: getting 5173 links ...\n",
      "Thread 48: finished !!!\n",
      "Thread 57: getting 5156 links ...\n",
      "Thread 49: finished !!!\n",
      "Thread 58: getting 5117 links ...\n",
      "Thread 53: finished !!!\n",
      "Thread 59: getting 5037 links ...\n",
      "Thread 55: finished !!!\n",
      "Thread 60: getting 5028 links ...\n",
      "Thread 52: finished !!!\n",
      "Thread 61: getting 5013 links ...\n",
      "Thread 54: finished !!!\n",
      "Thread 62: getting 5008 links ...\n",
      "Thread 51: finished !!!\n",
      "Thread 63: getting 4967 links ...\n",
      "Thread 58: finished !!!\n",
      "Thread 64: getting 4836 links ...\n",
      "Thread 57: finished !!!\n",
      "Thread 65: getting 4830 links ...\n",
      "Thread 56: finished !!!\n",
      "Thread 66: getting 4775 links ...\n",
      "Thread 59: finished !!!\n",
      "Thread 67: getting 4769 links ...\n",
      "Thread 62: finished !!!\n",
      "Thread 68: getting 4628 links ...\n",
      "Thread 60: finished !!!\n",
      "Thread 69: getting 4596 links ...\n",
      "Thread 63: finished !!!\n",
      "Thread 70: getting 4548 links ...\n",
      "Thread 65: finished !!!\n",
      "Thread 64: finished !!!\n",
      "Thread 66: finished !!!\n",
      "Thread 61: finished !!!\n",
      "Thread 67: finished !!!\n",
      "Thread 68: finished !!!\n",
      "Thread 69: finished !!!\n",
      "Thread 70: finished !!!\n",
      "!!!!!! Finished Scraping Links !!!!!!\n",
      "for-rent data len: 76592\n",
      "for-sale data len: 34367\n"
     ]
    }
   ],
   "source": [
    "num = 3\n",
    "input_path = '/kaggle/input/ph-1-getting-locations'\n",
    "file_name = 'locations.csv'\n",
    "locations = pd.read_csv(f\"{input_path}/{file_name}\")\n",
    "locations = locations[locations['flag']==num]\n",
    "\n",
    "thread_num = len(locations)\n",
    "print(f'Number of threads: {thread_num}')\n",
    "getlink_threads = [GetLinks() for _ in range(thread_num)]\n",
    "[getlink_thread.import_shuffle_df(f'{input_path}/{file_name}',num) for getlink_thread in getlink_threads]\n",
    "\n",
    "threadList = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for i in range(thread_num):\n",
    "        threadList.append(executor.submit(getlink_threads[i].get_condo_links, i+1))\n",
    "wait(threadList);\n",
    "\n",
    "getlink_threads[0].join_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7745.53083,
   "end_time": "2023-06-30T11:35:21.632783",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-30T09:26:16.101953",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
