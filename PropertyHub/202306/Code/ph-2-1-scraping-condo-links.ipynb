{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2358bc",
   "metadata": {
    "papermill": {
     "duration": 0.002708,
     "end_time": "2023-06-30T09:26:22.025393",
     "exception": false,
     "start_time": "2023-06-30T09:26:22.022685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PropertyHub: 2.1 Scraping Condo Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e902aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T09:26:22.031311Z",
     "iopub.status.busy": "2023-06-30T09:26:22.030795Z",
     "iopub.status.idle": "2023-06-30T09:26:22.218606Z",
     "shell.execute_reply": "2023-06-30T09:26:22.217652Z"
    },
    "papermill": {
     "duration": 0.19352,
     "end_time": "2023-06-30T09:26:22.221072",
     "exception": false,
     "start_time": "2023-06-30T09:26:22.027552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7096de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T09:26:22.226653Z",
     "iopub.status.busy": "2023-06-30T09:26:22.226106Z",
     "iopub.status.idle": "2023-06-30T09:26:22.372074Z",
     "shell.execute_reply": "2023-06-30T09:26:22.371108Z"
    },
    "papermill": {
     "duration": 0.151233,
     "end_time": "2023-06-30T09:26:22.374399",
     "exception": false,
     "start_time": "2023-06-30T09:26:22.223166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GetLinks():\n",
    "    def __init__(self):\n",
    "        self.delay_time = 0.1\n",
    "        \n",
    "        self.rent_place_type = []\n",
    "        self.rent_place_name = []\n",
    "        self.rent_link_name = []\n",
    "        self.rent_condo_links = []\n",
    "        self.sale_place_type = []\n",
    "        self.sale_place_name = []\n",
    "        self.sale_link_name = []\n",
    "        self.sale_condo_links = []\n",
    "        \n",
    "        self.retries = 5\n",
    "        self.backoff = 1     # time-out = [0.5, 1, 2, 4, 8]\n",
    "        self.status_forcelist = [403, 500, 502, 503, 504]\n",
    "        self.timeout = (10, 10)\n",
    "        \n",
    "        # Initialize request session for retries and timeout\n",
    "        self.s = requests.Session()\n",
    "        retries = Retry(total=self.retries,\n",
    "                        backoff_factor=self.backoff,\n",
    "                        status_forcelist=self.status_forcelist)\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    def import_shuffle_df(self, path, num):\n",
    "        self.location = pd.read_csv(path)\n",
    "        self.location = self.location[self.location['flag']==num]\n",
    "        \n",
    "    def get_condo_links(self, order):\n",
    "        df_place_link_name = self.location.iloc[order-1,:]\n",
    "        place_link_name = df_place_link_name['link_name']\n",
    "        \n",
    "        print(f\"Thread {order}: getting {df_place_link_name['num_rent']+df_place_link_name['num_sale']} links ...\")\n",
    "        \n",
    "        # Get for-rent links\n",
    "        if df_place_link_name['num_rent'] != 0:\n",
    "            try:\n",
    "                place_link = f'https://propertyhub.in.th/en/condo-for-rent/{place_link_name}'\n",
    "                # find max page number\n",
    "                soup = BeautifulSoup(self.s.get(place_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                find_max_page = soup.find_all(\"ul\", {\"class\": \"sc-1p20b44-0 IoRRS\"})\n",
    "                try:\n",
    "                    max_page = int(find_max_page[0].find_all('li')[-2].a['aria-label'].split()[-1])\n",
    "                except:\n",
    "                    max_page = 1\n",
    "                page_links = [place_link + f'/{i+1}' if i!=0 else place_link for i in range(max_page)]\n",
    "                for page_link in page_links:\n",
    "                    soup = BeautifulSoup(self.s.get(page_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                    find_condo_links = soup.select(\"a[href*='en/listings/']\")\n",
    "                    for link in find_condo_links:\n",
    "                        condo_link = 'https://propertyhub.in.th' + link['href']\n",
    "                        self.rent_place_type.append(df_place_link_name['place_type'])\n",
    "                        self.rent_place_name.append(df_place_link_name['place_name'])\n",
    "                        self.rent_link_name.append(df_place_link_name['link_name'])\n",
    "                        self.rent_condo_links.append(condo_link)\n",
    "                time.sleep(self.delay_time)\n",
    "            except:\n",
    "                print(f\"There is an invalid link: {place_link}\")\n",
    "\n",
    "        # Get for-sale links\n",
    "        if df_place_link_name['num_sale'] != 0:\n",
    "            try:\n",
    "                place_link = f'https://propertyhub.in.th/en/condo-for-sale/{place_link_name}'\n",
    "                # find max page number\n",
    "                soup = BeautifulSoup(self.s.get(place_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                find_max_page = soup.find_all(\"ul\", {\"class\": \"sc-1p20b44-0 IoRRS\"})\n",
    "                try:\n",
    "                    max_page = int(find_max_page[0].find_all('li')[-2].a['aria-label'].split()[-1])\n",
    "                except:\n",
    "                    max_page = 1\n",
    "                page_links = [place_link + f'/{i+1}' if i!=0 else place_link for i in range(max_page)]\n",
    "                for page_link in page_links:\n",
    "                    soup = BeautifulSoup(self.s.get(page_link, timeout=self.timeout).content, \"html.parser\")\n",
    "                    find_condo_links = soup.select(\"a[href*='en/listings/']\")\n",
    "                    for link in find_condo_links:\n",
    "                        condo_link = 'https://propertyhub.in.th' + link['href']\n",
    "                        self.sale_place_type.append(df_place_link_name['place_type'])\n",
    "                        self.sale_place_name.append(df_place_link_name['place_name'])\n",
    "                        self.sale_link_name.append(df_place_link_name['link_name'])\n",
    "                        self.sale_condo_links.append(condo_link)\n",
    "                time.sleep(self.delay_time)\n",
    "            except:\n",
    "                print(f\"There is an invalid link: {place_link}\")\n",
    "\n",
    "        # Convert to df and remove duplicates\n",
    "        self.df_rent_condo_links = pd.DataFrame(self.rent_place_type,columns=['place_type'])\n",
    "        self.df_rent_condo_links['place_name'] = self.rent_place_name\n",
    "        self.df_rent_condo_links['link_name'] = self.rent_link_name\n",
    "        self.df_rent_condo_links['condo_link'] = self.rent_condo_links\n",
    "        self.df_rent_condo_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "        self.df_sale_condo_links = pd.DataFrame(self.sale_place_type,columns=['place_type'])\n",
    "        self.df_sale_condo_links['place_name'] = self.sale_place_name\n",
    "        self.df_sale_condo_links['link_name'] = self.sale_link_name\n",
    "        self.df_sale_condo_links['condo_link'] = self.sale_condo_links\n",
    "        self.df_sale_condo_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "        # Export\n",
    "        self.df_rent_condo_links.to_csv(f\"df_rent_condo_links_{order}.csv\",index=False)\n",
    "        self.df_sale_condo_links.to_csv(f\"df_sale_condo_links_{order}.csv\",index=False)\n",
    "        \n",
    "        print(f\"Thread {order}: finished !!!\")\n",
    "            \n",
    "    def join_df(self):\n",
    "        output_rent_files = glob.glob(os.getcwd()+'/df_rent*.csv')\n",
    "        output_sale_files = glob.glob(os.getcwd()+'/df_sale*.csv')\n",
    "        outputs_rent = [pd.read_csv(output_rent_file) for output_rent_file in output_rent_files]\n",
    "        outputs_sale = [pd.read_csv(output_sale_file) for output_sale_file in output_sale_files]\n",
    "\n",
    "        # Combine outputs\n",
    "        output_rent_all = outputs_rent[0]\n",
    "        for i in range(len(outputs_rent)-1):\n",
    "            output_rent_all = pd.concat([output_rent_all,outputs_rent[i+1]], axis=0, ignore_index=True)\n",
    "        output_sale_all = outputs_sale[0]\n",
    "        for i in range(len(outputs_sale)-1):\n",
    "            output_sale_all = pd.concat([output_sale_all,outputs_sale[i+1]], axis=0, ignore_index=True)\n",
    "            \n",
    "        # Drop duplicates\n",
    "        output_rent_all.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "        output_sale_all.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'!!!!!! Finished Scraping Links !!!!!!')\n",
    "        print(f'for-rent data len: {len(output_rent_all)}')\n",
    "        print(f'for-sale data len: {len(output_sale_all)}')\n",
    "\n",
    "        # Export the combine result\n",
    "        output_rent_all.to_csv(f\"{datetime.now().strftime('%Y%m')}_rent_condo_links_1.csv\",index=False)\n",
    "        output_sale_all.to_csv(f\"{datetime.now().strftime('%Y%m')}_sale_condo_links_1.csv\",index=False)\n",
    "\n",
    "        # Delete all the unused files\n",
    "        [os.remove(output_rent_file) for output_rent_file in output_rent_files];\n",
    "        [os.remove(output_sale_file) for output_sale_file in output_sale_files];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62082d61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T09:26:22.380346Z",
     "iopub.status.busy": "2023-06-30T09:26:22.379969Z",
     "iopub.status.idle": "2023-06-30T11:51:46.411353Z",
     "shell.execute_reply": "2023-06-30T11:51:46.409634Z"
    },
    "papermill": {
     "duration": 8724.038085,
     "end_time": "2023-06-30T11:51:46.414542",
     "exception": false,
     "start_time": "2023-06-30T09:26:22.376457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of threads: 15\n",
      "Thread 1: getting 54362 links ...\n",
      "Thread 2: getting 52825 links ...\n",
      "Thread 3: getting 52565 links ...\n",
      "Thread 4: getting 50361 links ...\n",
      "Thread 5: getting 45332 links ...\n",
      "Thread 6: getting 38338 links ...\n",
      "Thread 7: getting 33768 links ...\n",
      "Thread 8: getting 27439 links ...\n",
      "Thread 8: finished !!!\n",
      "Thread 9: getting 25291 links ...\n",
      "Thread 7: finished !!!\n",
      "Thread 10: getting 23093 links ...\n",
      "Thread 6: finished !!!\n",
      "Thread 11: getting 21681 links ...\n",
      "Thread 5: finished !!!\n",
      "Thread 12: getting 19349 links ...\n",
      "Thread 9: finished !!!\n",
      "Thread 13: getting 18733 links ...\n",
      "Thread 4: finished !!!\n",
      "Thread 14: getting 18623 links ...\n",
      "Thread 10: finished !!!\n",
      "Thread 15: getting 18520 links ...\n",
      "Thread 3: finished !!!\n",
      "Thread 2: finished !!!\n",
      "Thread 1: finished !!!\n",
      "Thread 11: finished !!!\n",
      "Thread 12: finished !!!\n",
      "Thread 13: finished !!!\n",
      "Thread 14: finished !!!\n",
      "Thread 15: finished !!!\n",
      "!!!!!! Finished Scraping Links !!!!!!\n",
      "for-rent data len: 60935\n",
      "for-sale data len: 33641\n"
     ]
    }
   ],
   "source": [
    "num = 1\n",
    "input_path = '/kaggle/input/ph-1-getting-locations'\n",
    "file_name = 'locations.csv'\n",
    "locations = pd.read_csv(f\"{input_path}/{file_name}\")\n",
    "locations = locations[locations['flag']==num]\n",
    "\n",
    "thread_num = len(locations)\n",
    "print(f'Number of threads: {thread_num}')\n",
    "getlink_threads = [GetLinks() for _ in range(thread_num)]\n",
    "[getlink_thread.import_shuffle_df(f'{input_path}/{file_name}',num) for getlink_thread in getlink_threads]\n",
    "\n",
    "threadList = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for i in range(thread_num):\n",
    "        threadList.append(executor.submit(getlink_threads[i].get_condo_links, i+1))\n",
    "wait(threadList);\n",
    "\n",
    "getlink_threads[0].join_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c85c9c",
   "metadata": {
    "papermill": {
     "duration": 0.00577,
     "end_time": "2023-06-30T11:51:46.424015",
     "exception": false,
     "start_time": "2023-06-30T11:51:46.418245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8735.325749,
   "end_time": "2023-06-30T11:51:47.869665",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-30T09:26:12.543916",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
