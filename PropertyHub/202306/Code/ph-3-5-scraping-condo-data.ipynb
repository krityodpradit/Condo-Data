{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e59700",
   "metadata": {
    "papermill": {
     "duration": 0.004424,
     "end_time": "2023-06-30T14:25:12.514914",
     "exception": false,
     "start_time": "2023-06-30T14:25:12.510490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PropertyHub: 3.5 Scraping Condo Data (for-sale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ddcd6f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-30T14:25:12.525514Z",
     "iopub.status.busy": "2023-06-30T14:25:12.524514Z",
     "iopub.status.idle": "2023-06-30T14:25:12.725188Z",
     "shell.execute_reply": "2023-06-30T14:25:12.723987Z"
    },
    "papermill": {
     "duration": 0.208517,
     "end_time": "2023-06-30T14:25:12.727877",
     "exception": false,
     "start_time": "2023-06-30T14:25:12.519360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ceaa1f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T14:25:12.735454Z",
     "iopub.status.busy": "2023-06-30T14:25:12.734865Z",
     "iopub.status.idle": "2023-06-30T14:25:12.890335Z",
     "shell.execute_reply": "2023-06-30T14:25:12.889111Z"
    },
    "papermill": {
     "duration": 0.163841,
     "end_time": "2023-06-30T14:25:12.894453",
     "exception": false,
     "start_time": "2023-06-30T14:25:12.730612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScrapeData():\n",
    "    def __init__(self):\n",
    "        self.delay_time = 0     # **** Please set this variable > 1 not to overload the server ****\n",
    "        self.condo_level_data = []\n",
    "        self.parse_count = 0\n",
    "        self.error_index = []\n",
    "        self.invalid_link_count = 0\n",
    "        self.retries = 5\n",
    "        self.backoff = 1     # time-out = [0.5, 1, 2, 4, 8]\n",
    "        self.status_forcelist = [403, 500, 502, 503, 504]\n",
    "        self.timeout = (10, 10)\n",
    "        \n",
    "        # Initialize request session for retries and timeout\n",
    "        self.s = requests.Session()\n",
    "        retries = Retry(total=self.retries,\n",
    "                        backoff_factor=self.backoff,\n",
    "                        status_forcelist=self.status_forcelist)\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "        \n",
    "    def parse_data(self):\n",
    "        for condo_link in self.condo_links:\n",
    "            try:\n",
    "                page = self.s.get(condo_link, timeout=self.timeout)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                # Parse condo details\n",
    "                post_name = soup.select(\"h1[class*='sc-14haut3-0 kiSLkD']\")[0].text.strip()\n",
    "                last_update_date = soup.select(\"div[class*='sc-ogfj7g-14 coaZDA']\")[0].text.strip().split()[-2]\n",
    "                last_update_time = soup.select(\"div[class*='sc-ogfj7g-14 coaZDA']\")[0].text.strip().split()[-1]\n",
    "                try:\n",
    "                    poster_name = soup.select(\"li[class*='sc-ves8oa-9 hCPUWp']\")[0].div.a.text\n",
    "                except:\n",
    "                    try:\n",
    "                        poster_name = soup.select(\"li[class*='sc-ves8oa-9 hCPUWp']\")[0].div.p.text\n",
    "                    except:\n",
    "                        poster_name = ''\n",
    "                poster_status = soup.select(\"li[class*='sc-ves8oa-9 hCPUWp']\")[0].div.div.text\n",
    "                # view_count = soup.select(\"div[class*='sc-ves8oa-0 hWLLWW']\")[0].text.strip().split()[-1]        # Cannot get view count as the page needs to load up\n",
    "                \n",
    "                if self.rent_flag == 1:\n",
    "                    price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[-1].text\n",
    "                    if price_unit.lower().strip() == 'please contact':\n",
    "                        try:\n",
    "                            price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[1].text.split('THB')[0].replace(',',''))\n",
    "                            price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[2].text.strip()\n",
    "                        except:\n",
    "                            price = 'please'\n",
    "                            price_unit = 'contact'\n",
    "                    else:\n",
    "                        price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[-2].text.split('THB')[0].replace(',',''))\n",
    "\n",
    "                    deposit = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[0].find_all('p')[-1].text.strip().split()[0]\n",
    "                    try:\n",
    "                        deposit_unit = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[0].find_all('p')[-1].text.strip().split()[1]\n",
    "                    except:\n",
    "                        deposit_unit = \"\"\n",
    "                    advance_payment = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[1].find_all('p')[-1].text.strip().split()[0]\n",
    "                    try:\n",
    "                        advance_payment_unit = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[1].find_all('p')[-1].text.strip().split()[1]\n",
    "                    except:\n",
    "                        advance_payment_unit = \"\"\n",
    "                else:\n",
    "                    price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[-1].text\n",
    "                    if price_unit.lower().strip() == 'please contact':\n",
    "                        try:\n",
    "                            price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[1].text.split('THB')[0].replace(',',''))\n",
    "                            price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[2].text.strip()\n",
    "                        except:\n",
    "                            price = 'please'\n",
    "                            price_unit = 'contact'\n",
    "                    else:\n",
    "                        price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[-2].text.split('THB')[0].replace(',',''))\n",
    "                    \n",
    "                    deposit = ''\n",
    "                    deposit_unit = ''\n",
    "                    advance_payment = ''\n",
    "                    advance_payment_unit = ''\n",
    "                    \n",
    "                room_info_header = [room_info.p.text.strip(':') for room_info in soup.select(\"li[class*='sc-s9r052-1 bLavUw']\")]\n",
    "                room_info_value = [room_info.span.text.strip() for room_info in soup.select(\"li[class*='sc-s9r052-1 bLavUw']\")]\n",
    "                find_room_description = soup.select(\"div[class*='sc-ves8oa-21 bBnKvP']\")\n",
    "                if len(find_room_description) == 0:\n",
    "                    room_description = []\n",
    "                else:\n",
    "                    room_description = find_room_description[0].text.strip()\n",
    "\n",
    "                room_amenities_have = [amen.text.strip() for amen in soup.select(\"div[class*='sc-1qj7qf1-1 czrvpe']\")[0].find_all('span')]\n",
    "                room_amenities_not_have = [amen.text.strip() for amen in soup.select(\"div[class*='sc-1qj7qf1-1 czrvpe']\")[0].find_all('strike')]\n",
    "\n",
    "                # Parse project details\n",
    "                project_details_attributes = [header.text.strip() for header in soup.select(\"table[class*='sc-7l0zor-1 jEPVvF']\")[0].find_all('th')]\n",
    "                project_details_values = []\n",
    "                for value in soup.select(\"table[class*='sc-7l0zor-1 jEPVvF']\")[0].find_all('td'):\n",
    "                    if len(value.find_all('li')) <= 1:\n",
    "                        project_details_values.append(value.text.strip())\n",
    "                    else:\n",
    "                        value_list = [v.text.strip() for v in value.find_all('li')]\n",
    "                        project_details_values.append(value_list)\n",
    "                facilities_have = [facil.text.strip() for facil in soup.select(\"div[class*='sc-vxzykp-0 dTLeQV sc-ogfj7g-18 sc-iv2rdv-17 bLEjbt citTje']\")[0].find_all('span')]\n",
    "                facilities_not_have = [facil.text.strip() for facil in soup.select(\"div[class*='sc-vxzykp-0 dTLeQV sc-ogfj7g-18 sc-iv2rdv-17 bLEjbt citTje']\")[0].find_all('strike')]\n",
    "\n",
    "                # Parse properties in nearby area\n",
    "                nearby_property_type = []\n",
    "                nearby_property_name = []\n",
    "                nearby_property_distance = []\n",
    "\n",
    "                blocks = soup.select(\"div[class*='sc-vxzykp-0 dNnjli sc-nnw194-1 bnkbhT']\")\n",
    "                blocks.extend(soup.select(\"div[class*='sc-vxzykp-0 dTLeQV sc-nnw194-1 bnkbhT']\"))\n",
    "\n",
    "                for block in blocks:\n",
    "                    for i,sub_block in enumerate(block.select(\"div[class*='row sc-nnw194-2 gAWLJy']\")):\n",
    "                        for prop in sub_block.select(\"a[class*='zoneTypeStyle']\"):\n",
    "                            nearby_property_type.append(block.find_all('h3')[i].text.strip())\n",
    "                            nearby_property_name.append(prop.text.strip().replace('Condo ',''))\n",
    "                            if len(sub_block.find_all('span'))==0:\n",
    "                                nearby_property_distance.append('')\n",
    "                            else:\n",
    "                                nearby_property_distance.append(sub_block.find_all('span')[0].text.strip())\n",
    "\n",
    "                self.condo_level_data.append([self.rent_flag,\n",
    "                                            condo_link,\n",
    "                                            post_name,\n",
    "                                            last_update_date,\n",
    "                                            last_update_time,\n",
    "                                            poster_name,\n",
    "                                            poster_status,\n",
    "                                            price,\n",
    "                                            price_unit,\n",
    "                                            deposit,\n",
    "                                            deposit_unit,\n",
    "                                            advance_payment,\n",
    "                                            advance_payment_unit,\n",
    "                                            room_info_header,\n",
    "                                            room_info_value,\n",
    "                                            room_description,\n",
    "                                            room_amenities_have,\n",
    "                                            room_amenities_not_have,\n",
    "                                            project_details_attributes,\n",
    "                                            project_details_values,\n",
    "                                            facilities_have,\n",
    "                                            facilities_not_have,\n",
    "                                            nearby_property_type,\n",
    "                                            nearby_property_name,\n",
    "                                            nearby_property_distance])\n",
    "\n",
    "                self.parse_count += 1\n",
    "                time.sleep(self.delay_time)\n",
    "                \n",
    "            except:\n",
    "                self.condo_level_data.append([])\n",
    "                if (page.url == 'https://propertyhub.in.th/en') or (len(soup.select(\"div[class*='sc-1552ugy-1 sc-1552ugy-5 kvqbdg eugViW']\")) != 0):\n",
    "                    self.invalid_link_count += 1 \n",
    "                    self.parse_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f'Error at {condo_link}')\n",
    "                    self.error_index.append(self.parse_count)\n",
    "                    self.parse_count += 1\n",
    "                    continue\n",
    "                \n",
    "    \n",
    "    def export_results(self):\n",
    "        condo_data = pd.DataFrame(self.condo_level_data)\n",
    "        col_names = ['rent_flag',\n",
    "                    'condo_link',\n",
    "                    'post_name',\n",
    "                    'last_update_date',\n",
    "                    'last_update_time',\n",
    "                    'poster_name',\n",
    "                    'poster_status',\n",
    "                    'price',\n",
    "                    'price_unit',\n",
    "                    'deposit',\n",
    "                    'deposit_unit',\n",
    "                    'advance_payment',\n",
    "                    'advance_payment_unit',\n",
    "                    'room_info_header',\n",
    "                    'room_info_value',\n",
    "                    'room_description',\n",
    "                    'room_amenities_have',\n",
    "                    'room_amenities_not_have',\n",
    "                    'project_details_attributes',\n",
    "                    'project_details_values',\n",
    "                    'facilities_have',\n",
    "                    'facilities_not_have',\n",
    "                    'nearby_property_type',\n",
    "                    'nearby_property_name',\n",
    "                    'nearby_property_distance']\n",
    "        \n",
    "        condo_data.columns = col_names\n",
    "        condo_data.to_csv(f\"condo_data_{self.order}.csv\",index=False)\n",
    "    \n",
    "    def main(self, df_links, order, rent_flag=1):\n",
    "        self.condo_links = df_links.iloc[:,3]\n",
    "        self.order = order\n",
    "        self.rent_flag = rent_flag   # 1 for-rent and 0 for-sale\n",
    "        \n",
    "        print(f'Thread {self.order}: Scraping for {len(self.condo_links)} links ...')\n",
    "        \n",
    "        self.parse_data()\n",
    "        self.export_results()\n",
    "        \n",
    "        if len(self.error_index) > 0:\n",
    "            print(f'Thread {self.order}: !!! Scraping completed ({len(self.error_index)} errors, {self.invalid_link_count} invalid links)')\n",
    "        else:\n",
    "            print(f'Thread {self.order}: !!! Scraping completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98e2640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T14:25:12.903866Z",
     "iopub.status.busy": "2023-06-30T14:25:12.903478Z",
     "iopub.status.idle": "2023-06-30T14:25:13.784482Z",
     "shell.execute_reply": "2023-06-30T14:25:13.783715Z"
    },
    "papermill": {
     "duration": 0.888272,
     "end_time": "2023-06-30T14:25:13.786691",
     "exception": false,
     "start_time": "2023-06-30T14:25:12.898419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st batch: for-sale data len: 26253\n"
     ]
    }
   ],
   "source": [
    "# Once scraping condo links notebook is done, upload the generated output files to this notebook and update the input path\n",
    "path_1 = '/kaggle/input/ph-2-1-scraping-condo-links'\n",
    "df_sale_links_1 = pd.read_csv(f\"{path_1}/{datetime.now().strftime('%Y%m')}_sale_condo_links_1.csv\")\n",
    "\n",
    "path_2 = '/kaggle/input/ph-2-2-scraping-condo-links'\n",
    "df_sale_links_2 = pd.read_csv(f\"{path_2}/{datetime.now().strftime('%Y%m')}_sale_condo_links_2.csv\")\n",
    "\n",
    "path_3 = '/kaggle/input/ph-2-3-scraping-condo-links'\n",
    "df_sale_links_3 = pd.read_csv(f\"{path_3}/{datetime.now().strftime('%Y%m')}_sale_condo_links_3.csv\")\n",
    "\n",
    "path_4 = '/kaggle/input/ph-2-4-scraping-condo-links'\n",
    "df_sale_links_4 = pd.read_csv(f\"{path_4}/{datetime.now().strftime('%Y%m')}_sale_condo_links_4.csv\")\n",
    "\n",
    "df_sale_links = pd.concat([df_sale_links_1,df_sale_links_2,df_sale_links_3,df_sale_links_4], axis=0, ignore_index=True)\n",
    "df_sale_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "# Dividing into groups\n",
    "sale_len = round(len(df_sale_links)/2)\n",
    "print(f'1st batch: for-sale data len: {sale_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a33a177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T14:25:13.793986Z",
     "iopub.status.busy": "2023-06-30T14:25:13.793521Z",
     "iopub.status.idle": "2023-06-30T19:10:24.677127Z",
     "shell.execute_reply": "2023-06-30T19:10:24.676070Z"
    },
    "papermill": {
     "duration": 17110.894068,
     "end_time": "2023-06-30T19:10:24.683629",
     "exception": false,
     "start_time": "2023-06-30T14:25:13.789561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 1: Scraping for 1641 links ...Thread 2: Scraping for 1641 links ...\n",
      "\n",
      "Thread 3: Scraping for 1640 links ...\n",
      "Thread 4: Scraping for 1641 links ...\n",
      "Thread 5: Scraping for 1641 links ...\n",
      "Thread 6: Scraping for 1641 links ...\n",
      "Thread 7: Scraping for 1641 links ...\n",
      "Thread 8: Scraping for 1640 links ...\n",
      "Error at https://propertyhub.in.th/en/listings/1-br-condo-at-voque-sukhumvit-16-condominium-near-mrt-queen-sirikit-national-convention-centre-id-883887---3566476\n",
      "Error at https://propertyhub.in.th/en/listings/bast-price-387k-sq-m-duplex-penthouse-2-beds-3-baths-with-bathtub-top-fl-bts-phloen-chit-at-noble-ploenchit-condo-for-sale---3452517\n",
      "Thread 2: !!! Scraping completed\n",
      "Thread 9: Scraping for 1641 links ...\n",
      "Thread 3: !!! Scraping completed\n",
      "Thread 10: Scraping for 1641 links ...\n",
      "Thread 1: !!! Scraping completed\n",
      "Thread 11: Scraping for 1641 links ...\n",
      "Thread 4: !!! Scraping completed\n",
      "Thread 12: Scraping for 1641 links ...\n",
      "Thread 5: !!! Scraping completed\n",
      "Thread 13: Scraping for 1641 links ...\n",
      "Thread 6: !!! Scraping completed\n",
      "Thread 14: Scraping for 1640 links ...\n",
      "Thread 7: !!! Scraping completed (1 errors, 0 invalid links)\n",
      "Thread 15: Scraping for 1641 links ...\n",
      "Thread 8: !!! Scraping completed (1 errors, 1 invalid links)\n",
      "Thread 16: Scraping for 1641 links ...\n",
      "Thread 9: !!! Scraping completed\n",
      "Thread 15: !!! Scraping completed\n",
      "Thread 16: !!! Scraping completed\n",
      "Thread 11: !!! Scraping completed\n",
      "Thread 10: !!! Scraping completed\n",
      "Thread 12: !!! Scraping completed\n",
      "Thread 14: !!! Scraping completed\n",
      "Thread 13: !!! Scraping completed\n"
     ]
    }
   ],
   "source": [
    "# Scraping for-sale condo data\n",
    "sale_group_num = 32\n",
    "sale_links = []\n",
    "for i in range(sale_group_num):\n",
    "    start = round(i*len(df_sale_links)/sale_group_num)\n",
    "    end = round((i+1)*len(df_sale_links)/sale_group_num)\n",
    "    sale_links.append(df_sale_links.iloc[start:end,:])\n",
    "\n",
    "sale_thread_num = 16\n",
    "sale_scrape_threads = [ScrapeData() for _ in range(sale_thread_num)]\n",
    "sale_threadList = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for i in range(sale_thread_num):\n",
    "        sale_threadList.append(executor.submit(sale_scrape_threads[i].main, sale_links[i], i+1, 0))\n",
    "wait(sale_threadList);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f93d76a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T19:10:24.693839Z",
     "iopub.status.busy": "2023-06-30T19:10:24.692742Z",
     "iopub.status.idle": "2023-06-30T19:10:29.790066Z",
     "shell.execute_reply": "2023-06-30T19:10:29.788825Z"
    },
    "papermill": {
     "duration": 5.105293,
     "end_time": "2023-06-30T19:10:29.792886",
     "exception": false,
     "start_time": "2023-06-30T19:10:24.687593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total output length: 26243\n"
     ]
    }
   ],
   "source": [
    "# Read all files in the output folder\n",
    "output_files = glob.glob(f\"{os.getcwd()}/condo_data*.csv\")\n",
    "outputs = [pd.read_csv(f'{output_file}') for output_file in output_files]\n",
    "\n",
    "# Combine outputs\n",
    "output_all = outputs[0]\n",
    "for i in range(len(outputs)-1):\n",
    "    output_all = pd.concat([output_all,outputs[i+1]], axis=0, ignore_index=True)\n",
    "\n",
    "# Drop empty rows resulting from errors from scraping\n",
    "output_all.dropna(subset=['post_name'], axis=0, inplace=True)\n",
    "output_all.reset_index(inplace=True, drop=True)\n",
    "output = output_all\n",
    "\n",
    "# Export the combine result\n",
    "output.to_csv(f\"{datetime.now().strftime('%Y%m')}_condo_data_5.csv\",index=False)\n",
    "\n",
    "# Delete all the unused files\n",
    "[os.remove(output_file) for output_file in output_files];\n",
    "\n",
    "# Show results\n",
    "print(f'Total output length: {len(output)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17129.196822,
   "end_time": "2023-06-30T19:10:31.750699",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-30T14:25:02.553877",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
