{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d38254b",
   "metadata": {
    "papermill": {
     "duration": 0.003613,
     "end_time": "2023-06-30T14:25:10.432303",
     "exception": false,
     "start_time": "2023-06-30T14:25:10.428690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PropertyHub: 3.4 Scraping Condo Data (for-rent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd03c0c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-30T14:25:10.440071Z",
     "iopub.status.busy": "2023-06-30T14:25:10.439691Z",
     "iopub.status.idle": "2023-06-30T14:25:10.759356Z",
     "shell.execute_reply": "2023-06-30T14:25:10.758289Z"
    },
    "papermill": {
     "duration": 0.326734,
     "end_time": "2023-06-30T14:25:10.762215",
     "exception": false,
     "start_time": "2023-06-30T14:25:10.435481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab55736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T14:25:10.770389Z",
     "iopub.status.busy": "2023-06-30T14:25:10.769889Z",
     "iopub.status.idle": "2023-06-30T14:25:10.814868Z",
     "shell.execute_reply": "2023-06-30T14:25:10.813899Z"
    },
    "papermill": {
     "duration": 0.052125,
     "end_time": "2023-06-30T14:25:10.817566",
     "exception": false,
     "start_time": "2023-06-30T14:25:10.765441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ScrapeData():\n",
    "    def __init__(self):\n",
    "        self.delay_time = 0     # **** Please set this variable > 1 not to overload the server ****\n",
    "        self.condo_level_data = []\n",
    "        self.parse_count = 0\n",
    "        self.error_index = []\n",
    "        self.invalid_link_count = 0\n",
    "        self.retries = 5\n",
    "        self.backoff = 1     # time-out = [0.5, 1, 2, 4, 8]\n",
    "        self.status_forcelist = [403, 500, 502, 503, 504]\n",
    "        self.timeout = (10, 10)\n",
    "        \n",
    "        # Initialize request session for retries and timeout\n",
    "        self.s = requests.Session()\n",
    "        retries = Retry(total=self.retries,\n",
    "                        backoff_factor=self.backoff,\n",
    "                        status_forcelist=self.status_forcelist)\n",
    "        self.s.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.s.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "        \n",
    "    def parse_data(self):\n",
    "        for condo_link in self.condo_links:\n",
    "            try:\n",
    "                page = self.s.get(condo_link, timeout=self.timeout)\n",
    "                soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "                # Parse condo details\n",
    "                post_name = soup.select(\"h1[class*='sc-14haut3-0 kiSLkD']\")[0].text.strip()\n",
    "                last_update_date = soup.select(\"div[class*='sc-ogfj7g-14 coaZDA']\")[0].text.strip().split()[-2]\n",
    "                last_update_time = soup.select(\"div[class*='sc-ogfj7g-14 coaZDA']\")[0].text.strip().split()[-1]\n",
    "                try:\n",
    "                    poster_name = soup.select(\"li[class*='sc-ves8oa-9 hCPUWp']\")[0].div.a.text\n",
    "                except:\n",
    "                    try:\n",
    "                        poster_name = soup.select(\"li[class*='sc-ves8oa-9 hCPUWp']\")[0].div.p.text\n",
    "                    except:\n",
    "                        poster_name = ''\n",
    "                poster_status = soup.select(\"li[class*='sc-ves8oa-9 hCPUWp']\")[0].div.div.text\n",
    "                # view_count = soup.select(\"div[class*='sc-ves8oa-0 hWLLWW']\")[0].text.strip().split()[-1]        # Cannot get view count as the page needs to load up\n",
    "                \n",
    "                if self.rent_flag == 1:\n",
    "                    price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[-1].text\n",
    "                    if price_unit.lower().strip() == 'please contact':\n",
    "                        try:\n",
    "                            price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[1].text.split('THB')[0].replace(',',''))\n",
    "                            price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[2].text.strip()\n",
    "                        except:\n",
    "                            price = 'please'\n",
    "                            price_unit = 'contact'\n",
    "                    else:\n",
    "                        price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-5 dvOoHM']\")[0].find_all('span')[-2].text.split('THB')[0].replace(',',''))\n",
    "\n",
    "                    deposit = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[0].find_all('p')[-1].text.strip().split()[0]\n",
    "                    try:\n",
    "                        deposit_unit = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[0].find_all('p')[-1].text.strip().split()[1]\n",
    "                    except:\n",
    "                        deposit_unit = \"\"\n",
    "                    advance_payment = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[1].find_all('p')[-1].text.strip().split()[0]\n",
    "                    try:\n",
    "                        advance_payment_unit = soup.select(\"li[class*='sc-s9r052-3 iJHhTM']\")[1].find_all('p')[-1].text.strip().split()[1]\n",
    "                    except:\n",
    "                        advance_payment_unit = \"\"\n",
    "                else:\n",
    "                    price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[-1].text\n",
    "                    if price_unit.lower().strip() == 'please contact':\n",
    "                        try:\n",
    "                            price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[1].text.split('THB')[0].replace(',',''))\n",
    "                            price_unit = soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[2].text.strip()\n",
    "                        except:\n",
    "                            price = 'please'\n",
    "                            price_unit = 'contact'\n",
    "                    else:\n",
    "                        price = int(soup.select(\"div[class*='sc-152o12i-7 dKuoZx priceTag sc-s9r052-6 gRLtIb']\")[0].find_all('span')[-2].text.split('THB')[0].replace(',',''))\n",
    "                    \n",
    "                    deposit = ''\n",
    "                    deposit_unit = ''\n",
    "                    advance_payment = ''\n",
    "                    advance_payment_unit = ''\n",
    "                    \n",
    "                room_info_header = [room_info.p.text.strip(':') for room_info in soup.select(\"li[class*='sc-s9r052-1 bLavUw']\")]\n",
    "                room_info_value = [room_info.span.text.strip() for room_info in soup.select(\"li[class*='sc-s9r052-1 bLavUw']\")]\n",
    "                find_room_description = soup.select(\"div[class*='sc-ves8oa-21 bBnKvP']\")\n",
    "                if len(find_room_description) == 0:\n",
    "                    room_description = []\n",
    "                else:\n",
    "                    room_description = find_room_description[0].text.strip()\n",
    "\n",
    "                room_amenities_have = [amen.text.strip() for amen in soup.select(\"div[class*='sc-1qj7qf1-1 czrvpe']\")[0].find_all('span')]\n",
    "                room_amenities_not_have = [amen.text.strip() for amen in soup.select(\"div[class*='sc-1qj7qf1-1 czrvpe']\")[0].find_all('strike')]\n",
    "\n",
    "                # Parse project details\n",
    "                project_details_attributes = [header.text.strip() for header in soup.select(\"table[class*='sc-7l0zor-1 jEPVvF']\")[0].find_all('th')]\n",
    "                project_details_values = []\n",
    "                for value in soup.select(\"table[class*='sc-7l0zor-1 jEPVvF']\")[0].find_all('td'):\n",
    "                    if len(value.find_all('li')) <= 1:\n",
    "                        project_details_values.append(value.text.strip())\n",
    "                    else:\n",
    "                        value_list = [v.text.strip() for v in value.find_all('li')]\n",
    "                        project_details_values.append(value_list)\n",
    "                facilities_have = [facil.text.strip() for facil in soup.select(\"div[class*='sc-vxzykp-0 dTLeQV sc-ogfj7g-18 sc-iv2rdv-17 bLEjbt citTje']\")[0].find_all('span')]\n",
    "                facilities_not_have = [facil.text.strip() for facil in soup.select(\"div[class*='sc-vxzykp-0 dTLeQV sc-ogfj7g-18 sc-iv2rdv-17 bLEjbt citTje']\")[0].find_all('strike')]\n",
    "\n",
    "                # Parse properties in nearby area\n",
    "                nearby_property_type = []\n",
    "                nearby_property_name = []\n",
    "                nearby_property_distance = []\n",
    "\n",
    "                blocks = soup.select(\"div[class*='sc-vxzykp-0 dNnjli sc-nnw194-1 bnkbhT']\")\n",
    "                blocks.extend(soup.select(\"div[class*='sc-vxzykp-0 dTLeQV sc-nnw194-1 bnkbhT']\"))\n",
    "\n",
    "                for block in blocks:\n",
    "                    for i,sub_block in enumerate(block.select(\"div[class*='row sc-nnw194-2 gAWLJy']\")):\n",
    "                        for prop in sub_block.select(\"a[class*='zoneTypeStyle']\"):\n",
    "                            nearby_property_type.append(block.find_all('h3')[i].text.strip())\n",
    "                            nearby_property_name.append(prop.text.strip().replace('Condo ',''))\n",
    "                            if len(sub_block.find_all('span'))==0:\n",
    "                                nearby_property_distance.append('')\n",
    "                            else:\n",
    "                                nearby_property_distance.append(sub_block.find_all('span')[0].text.strip())\n",
    "\n",
    "                self.condo_level_data.append([self.rent_flag,\n",
    "                                            condo_link,\n",
    "                                            post_name,\n",
    "                                            last_update_date,\n",
    "                                            last_update_time,\n",
    "                                            poster_name,\n",
    "                                            poster_status,\n",
    "                                            price,\n",
    "                                            price_unit,\n",
    "                                            deposit,\n",
    "                                            deposit_unit,\n",
    "                                            advance_payment,\n",
    "                                            advance_payment_unit,\n",
    "                                            room_info_header,\n",
    "                                            room_info_value,\n",
    "                                            room_description,\n",
    "                                            room_amenities_have,\n",
    "                                            room_amenities_not_have,\n",
    "                                            project_details_attributes,\n",
    "                                            project_details_values,\n",
    "                                            facilities_have,\n",
    "                                            facilities_not_have,\n",
    "                                            nearby_property_type,\n",
    "                                            nearby_property_name,\n",
    "                                            nearby_property_distance])\n",
    "\n",
    "                self.parse_count += 1\n",
    "                time.sleep(self.delay_time)\n",
    "                \n",
    "            except:\n",
    "                self.condo_level_data.append([])\n",
    "                if (page.url == 'https://propertyhub.in.th/en') or (len(soup.select(\"div[class*='sc-1552ugy-1 sc-1552ugy-5 kvqbdg eugViW']\")) != 0):\n",
    "                    self.invalid_link_count += 1 \n",
    "                    self.parse_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f'Error at {condo_link}')\n",
    "                    self.error_index.append(self.parse_count)\n",
    "                    self.parse_count += 1\n",
    "                    continue\n",
    "                \n",
    "    \n",
    "    def export_results(self):\n",
    "        condo_data = pd.DataFrame(self.condo_level_data)\n",
    "        col_names = ['rent_flag',\n",
    "                    'condo_link',\n",
    "                    'post_name',\n",
    "                    'last_update_date',\n",
    "                    'last_update_time',\n",
    "                    'poster_name',\n",
    "                    'poster_status',\n",
    "                    'price',\n",
    "                    'price_unit',\n",
    "                    'deposit',\n",
    "                    'deposit_unit',\n",
    "                    'advance_payment',\n",
    "                    'advance_payment_unit',\n",
    "                    'room_info_header',\n",
    "                    'room_info_value',\n",
    "                    'room_description',\n",
    "                    'room_amenities_have',\n",
    "                    'room_amenities_not_have',\n",
    "                    'project_details_attributes',\n",
    "                    'project_details_values',\n",
    "                    'facilities_have',\n",
    "                    'facilities_not_have',\n",
    "                    'nearby_property_type',\n",
    "                    'nearby_property_name',\n",
    "                    'nearby_property_distance']\n",
    "        \n",
    "        condo_data.columns = col_names\n",
    "        condo_data.to_csv(f\"condo_data_{self.order}.csv\",index=False)\n",
    "    \n",
    "    def main(self, df_links, order, rent_flag=1):\n",
    "        self.condo_links = df_links.iloc[:,3]\n",
    "        self.order = order\n",
    "        self.rent_flag = rent_flag   # 1 for-rent and 0 for-sale\n",
    "        \n",
    "        print(f'Thread {self.order}: Scraping for {len(self.condo_links)} links ...')\n",
    "        \n",
    "        self.parse_data()\n",
    "        self.export_results()\n",
    "        \n",
    "        if len(self.error_index) > 0:\n",
    "            print(f'Thread {self.order}: !!! Scraping completed ({len(self.error_index)} errors, {self.invalid_link_count} invalid links)')\n",
    "        else:\n",
    "            print(f'Thread {self.order}: !!! Scraping completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f08345b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T14:25:10.825372Z",
     "iopub.status.busy": "2023-06-30T14:25:10.825034Z",
     "iopub.status.idle": "2023-06-30T14:25:12.488533Z",
     "shell.execute_reply": "2023-06-30T14:25:12.487298Z"
    },
    "papermill": {
     "duration": 1.671408,
     "end_time": "2023-06-30T14:25:12.492490",
     "exception": false,
     "start_time": "2023-06-30T14:25:10.821082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4th batch: for-rent data len: 26553\n"
     ]
    }
   ],
   "source": [
    "# Once scraping condo links notebook is done, upload the generated output files to this notebook and update the input path\n",
    "path_1 = '/kaggle/input/ph-2-1-scraping-condo-links'\n",
    "df_rent_links_1 = pd.read_csv(f\"{path_1}/{datetime.now().strftime('%Y%m')}_rent_condo_links_1.csv\")\n",
    "\n",
    "path_2 = '/kaggle/input/ph-2-2-scraping-condo-links'\n",
    "df_rent_links_2 = pd.read_csv(f\"{path_2}/{datetime.now().strftime('%Y%m')}_rent_condo_links_2.csv\")\n",
    "\n",
    "path_3 = '/kaggle/input/ph-2-3-scraping-condo-links'\n",
    "df_rent_links_3 = pd.read_csv(f\"{path_3}/{datetime.now().strftime('%Y%m')}_rent_condo_links_3.csv\")\n",
    "\n",
    "path_4 = '/kaggle/input/ph-2-4-scraping-condo-links'\n",
    "df_rent_links_4 = pd.read_csv(f\"{path_4}/{datetime.now().strftime('%Y%m')}_rent_condo_links_4.csv\")\n",
    "\n",
    "df_rent_links = pd.concat([df_rent_links_1,df_rent_links_2,df_rent_links_3,df_rent_links_4], axis=0, ignore_index=True)\n",
    "df_rent_links.drop_duplicates(subset=['condo_link'],keep='first',inplace=True, ignore_index=True)\n",
    "\n",
    "# Dividing into groups\n",
    "rent_len = round(len(df_rent_links)/4)\n",
    "print(f'4th batch: for-rent data len: {rent_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6279600b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T14:25:12.500376Z",
     "iopub.status.busy": "2023-06-30T14:25:12.500047Z",
     "iopub.status.idle": "2023-06-30T19:07:10.857860Z",
     "shell.execute_reply": "2023-06-30T19:07:10.856541Z"
    },
    "papermill": {
     "duration": 16918.364342,
     "end_time": "2023-06-30T19:07:10.860257",
     "exception": false,
     "start_time": "2023-06-30T14:25:12.495915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 49: Scraping for 1660 links ...\n",
      "Thread 50: Scraping for 1659 links ...\n",
      "Thread 51: Scraping for 1660 links ...\n",
      "Thread 52: Scraping for 1659 links ...\n",
      "Thread 53: Scraping for 1660 links ...\n",
      "Thread 54: Scraping for 1659 links ...\n",
      "Thread 55: Scraping for 1660 links ...\n",
      "Thread 56: Scraping for 1660 links ...\n",
      "Error at https://propertyhub.in.th/en/listings/for-rent-lumpini-ville-sukhumvit109-bearing-1-bed-26-5-sq-m-7th-floor---489930\n",
      "Error at https://propertyhub.in.th/en/listings/rich-park-triple-station--b45f09ef---3745410\n",
      "Error at https://propertyhub.in.th/en/listings/for-rent-rye-sukhumvit-101-1-nice-room-separate-kitchen-7th-floor---3774220\n",
      "Thread 51: !!! Scraping completed\n",
      "Thread 57: Scraping for 1659 links ...\n",
      "Thread 54: !!! Scraping completed\n",
      "Thread 58: Scraping for 1660 links ...\n",
      "Thread 52: !!! Scraping completed (1 errors, 3 invalid links)\n",
      "Thread 59: Scraping for 1659 links ...\n",
      "Thread 56: !!! Scraping completed\n",
      "Thread 60: Scraping for 1660 links ...\n",
      "Thread 53: !!! Scraping completed (1 errors, 2 invalid links)\n",
      "Thread 61: Scraping for 1659 links ...\n",
      "Thread 50: !!! Scraping completed\n",
      "Thread 62: Scraping for 1660 links ...\n",
      "Thread 49: !!! Scraping completed\n",
      "Thread 63: Scraping for 1659 links ...\n",
      "Thread 55: !!! Scraping completed (1 errors, 0 invalid links)\n",
      "Thread 64: Scraping for 1660 links ...\n",
      "Thread 63: !!! Scraping completed\n",
      "Thread 57: !!! Scraping completed\n",
      "Thread 58: !!! Scraping completed\n",
      "Thread 64: !!! Scraping completed\n",
      "Thread 60: !!! Scraping completed\n",
      "Thread 61: !!! Scraping completed\n",
      "Thread 62: !!! Scraping completed\n",
      "Thread 59: !!! Scraping completed\n"
     ]
    }
   ],
   "source": [
    "# Scraping for-rent condo data\n",
    "rent_group_num = 64\n",
    "rent_links = []\n",
    "for i in range(rent_group_num):\n",
    "    start = round(i*len(df_rent_links)/rent_group_num)\n",
    "    end = round((i+1)*len(df_rent_links)/rent_group_num)\n",
    "    rent_links.append(df_rent_links.iloc[start:end,:])\n",
    "\n",
    "rent_thread_num = 16\n",
    "rent_scrape_threads = [ScrapeData() for _ in range(rent_thread_num)]\n",
    "rent_threadList = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for i in range(rent_thread_num):\n",
    "        rent_threadList.append(executor.submit(rent_scrape_threads[i].main, rent_links[i+48], i+1+48, 1))\n",
    "wait(rent_threadList);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871d6d01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-30T19:07:10.871860Z",
     "iopub.status.busy": "2023-06-30T19:07:10.871490Z",
     "iopub.status.idle": "2023-06-30T19:07:15.256729Z",
     "shell.execute_reply": "2023-06-30T19:07:15.255723Z"
    },
    "papermill": {
     "duration": 4.393978,
     "end_time": "2023-06-30T19:07:15.259333",
     "exception": false,
     "start_time": "2023-06-30T19:07:10.865355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total output length: 26414\n"
     ]
    }
   ],
   "source": [
    "# Read all files in the output folder\n",
    "output_files = glob.glob(f\"{os.getcwd()}/condo_data*.csv\")\n",
    "outputs = [pd.read_csv(f'{output_file}') for output_file in output_files]\n",
    "\n",
    "# Combine outputs\n",
    "output_all = outputs[0]\n",
    "for i in range(len(outputs)-1):\n",
    "    output_all = pd.concat([output_all,outputs[i+1]], axis=0, ignore_index=True)\n",
    "\n",
    "# Drop empty rows resulting from errors from scraping\n",
    "output_all.dropna(subset=['post_name'], axis=0, inplace=True)\n",
    "output_all.reset_index(inplace=True, drop=True)\n",
    "output = output_all\n",
    "\n",
    "# Export the combine result\n",
    "output.to_csv(f\"{datetime.now().strftime('%Y%m')}_condo_data_4.csv\",index=False)\n",
    "\n",
    "# Delete all the unused files\n",
    "[os.remove(output_file) for output_file in output_files];\n",
    "\n",
    "# Show results\n",
    "print(f'Total output length: {len(output)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16938.040195,
   "end_time": "2023-06-30T19:07:16.926645",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-30T14:24:58.886450",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
